---
title: "Quantifying Infants' Everyday Restrained Experiences in the Home Using Wearable Inertial Sensors"
blank-lines-above-title: 2
shorttitle: "Quantify infant restraint with sensors"
fig-dpi: 300
execute: 
  echo: false
  warning: false
  error: false
  cache: false
author:
  - name: Hanzhi Wang
    orcid: 0009-0000-0520-5935
    email: hanzhi.wang@email.ucr.edu
    affiliations:
      - id: id1
        name: University of California, Riverside
        department: Department of Psychology
  - name: Hailey N. Rousey
    orcid: 0009-0000-9828-0693
    email: hailey.rousey@email.ucr.edu
    affiliations:
      - id: id1
        name: University of California, Riverside
        department: Department of Psychology
  - name: John M. Franchak
    corresponding: true
    orcid: 0000-0002-0751-2864
    email: franchak@ucr.edu
    affiliations:
      - id: id1
        name: University of California, Riverside
        department: Department of Psychology
      
author-note:
  blank-lines-above-author-note: 1
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Acknowledge and cite data/materials to be shared.
    # Example. Because the authors are equal contributors, order of authorship was determined by a fair coin toss.
    # Example: This study was registered at ClinicalTrials.gov (Identifier NTC998877).
    study-registration: null
    data-sharing: null
    # Example: This article is based on data published in Pulaski (2017).
    # Example: This article is based on the dissertation completed by Graham (2018).    
    # Example: Sally Jones earns royalties from the sale of Test X.
    conflict-of-interest: null
    # Example: This study was supported by Grant A123 from the National Science Foundation.
    financial-support: null
    # Example: The authors are grateful for the technical assistance of Dr. X during the initial design and setup of our lab equipment.
    gratitude: null
    authorship-agreements: null
abstract: "Physical restraint—including being held, carried, and restrained in devices—is a common feature of infants’ everyday lives. This study developed and validated a machine learning model to detect infants’ restraint using wearable sensors movement data derived from an existing data set (Franchak, 2023). The model achieved high accuracy (89%) and substantial kappa agreement (0.73) compared with human-coded ground truth. The model presented a slight bias towards overestimating unrestrained periods compared to restrained periods, but the bias was mitigated when using a longer data-aggregating window length. The model also showed convergent validity by corroborating prior studies that showed an age-related decrease in infant restraint time. In short, the current study demonstrated the feasibility of using wearable sensors to quantify infants’ real-world restraint experiences, offering a new tool for studying how daily restraint influences early development."
keywords: [infant, restraint, everyday experiences, wearable sensors, machine learning]
bibliography: [references.bib]
format:
  apaquarto-docx: 
    floatsintext: false
  apaquarto-html: default
  apaquarto-typst: default
  apaquarto-pdf: 
    # can be jou (journal), man (manuscript), stu (student), or doc (document)
    # for now, tables and figures do not render properly in jou mode. 
    # can be 10pt, 11pt, or 12pt
    # Integrate tables and figures in text
    # a4 paper if true, letter size if false
    # suppresses loading of the lmodern font package
    # Suppresses loading of the fontenc package
    # Suppress the title above the introduction
    # In jou mode, use times or pslatex instead of txfonts
    # In jou mode, use Computer Modern font instead of times
    # In jou mode, cancels automatic stretching of tables to column width 
    # Uses Helvetica font in stu and man mode
    # In man and stu mode, neutralizes the \helvetica command
    # In man and stu mode, uses typewriter font
    # Puts draft watermark on first page
    # Puts draft watermark on all pages
    # Masks references that are marked as the author's own
    # Hides correspondence text
    documentmode: man
    fontsize: 12pt
    floatsintext: false
    a4paper: false
    nolmodern: false
    nofontenc: false
    donotrepeattitle: false
    notxfonts: false
    notimes: false
    notab: false
    helv: false
    nosf: false
    tt: false
    draftfirst: false
    draftall: false
    mask: false
    journal: null
    volume: null
    course: null
    professor: null
    duedate: null
    nocorrespondence: false
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
library(papaja)
library(knitr)
library(patchwork)
library(scales)
library(ggforce)
library(hms)
library(tidyverse)
library(lubridate)
library(rstatix)
library(janitor)
library(flextable)
library(effectsize)
library(ggbeeswarm)
library(DescTools)
```

```{r}
#| include: false

set.seed(2025)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

theme_update(text = element_text(size = 12),
             axis.text.x = element_text(size = 12, color = "black"), 
             axis.title.x = element_text(size = 14),
             axis.text.y = element_text(size = 12,  color = "black"), 
             axis.title.y = element_text(size = 14), 
             panel.background = element_blank(),panel.border = element_blank(), 
             panel.grid.major = element_blank(),
             panel.grid.minor = element_blank(), axis.line = element_blank(), 
             axis.ticks.length=unit(.25, "cm"), 
             legend.key = element_rect(fill = "white")) 

model_labels <- c("4_no_pos", "16_no_pos", "30_no_pos")
model_shapes <- c(5,1,2) %>% set_names(model_labels)
model_colors <- c("#0072B2","#E69F00", "#009E73") %>% set_names(c("4 s", "16 s", "30 s"))
class_labels <- c("r", "u")
class_shapes <- c(5,1) %>% set_names(class_labels)
# class_colors <- c("#0072B2","#E69F00") %>% set_names(class_labels)

indx <- c("accuracy","kappa", "sensitivity","pos_pred_value","f1")
per_class_indx <- c("sensitivity","pos_pred_value","f1")

```

```{r}
# demographics
load("whole-model-prediction.RData")

ds_sum <- ds_sum %>% 
  filter(unique_id != "185/3")
ds_human <- ds_human %>% 
  filter(unique_id != "185/3")
id_info <- id_info %>% 
  filter(unique_id != "185/3")
id_demo <- id_demo %>% 
  filter(unique_id != "185/3")

age <- id_demo %>% 
  group_by(age_group) %>% 
  summarize(n=n(),
            .groups="drop")

sex<-id_demo %>% 
  group_by(infant_sex) %>% 
  summarize(n=n(),
            .groups="drop")

race<-id_demo %>% 
  group_by(infant_race) %>% 
  summarize(n=n(),
            .groups="drop")

```

```{r}
# Human coding reliability
colnames(ds_human) <- c("unique_id","agree","kappa")
library(psych)
stats_human <- ds_human %>% 
  summarize(mean_agree = mean(agree),
            min_agree = min(agree),
            max_agree = max(agree),
            mean_kappa = mean(kappa),
            min_kappa = min(kappa),
            max_kappa = max(kappa))
```

Physical restraint is a common feature in infants’ everyday lives. Infants are routinely held or carried by caregivers, and infants are often restrained in devices such as highchairs, walkers, and strollers [@Birken2015s]. As an externally regulated aspect of infants’ daily life, restraint shapes infants’ experiences based on caregivers’ decisions. Restraint limits infants’ physical activities and therefore hinders infants’ motor development [e.g., @Pin2007z]. Restraint also frames infants’ accessible physical environment by altering experiences that are relevant for perceptual and cognitive development. For example, restraint is associated with fewer object holding experiences [@Franchak2024l], reduced amount and quality of language input [@Malachowski2023a], and increased opportunities for joint attention [@Karasik2025h]. Therefore, characterizing infants’ restrained experiences helps profile infants’ daily learning contexts and situate their motor, perceptual, and cognitive development within everyday experiences.

Previous studies have used a variety of methods to document infants’ restrained time in the home. Some used parent surveys, including questionnaires, diaries, or ecological momentary assessment [e.g., @Abbott2001u; @Karasik2018t; @Franchak2019o]. Some used video recording and human annotation [e.g., @Springfield2025p; @Wang2025v]. Although surveys provide an overall profile of infants’ daily life, and video recordings provide continuous time series data, no method so far can simultaneously provide full-day and continuous data to account for infants’ restraint experiences. In the current study, we propose that wearable sensors can be a solution for recording full-day, continuous movement behavior, and machine learning algorithms can be an accurate and efficient way to automatically classify wearable sensor data.

Moreover, previous studies did not define restraint consistently. Some definitions only focused on caregiver restraint such as holding and carrying [e.g., @Airaksinen2024z; @Franchak2021k; @Franchak2024e; @Yao2019q], some only focused on seating devices [e.g., @Callahan1997s], some looked at different surfaces (e.g., lap, cradle, and seat) infants are on [e.g., @Graciosa2024o], some focused broadly on containment equipment [e.g., @Bartlett2003w; @Carson2022n; @Hesketh2015i; @Karasik2022a], some combined holding and carrying with seating equipment [@Springfield2025p], and others covered a wider range of restraint like furniture placement, seating devices and holding [@Franchak2024l; @Malachowski2023a; @Siddicky2021c]. The inconsistency in restraint definitions makes it difficult to compare results across studies. In the current study, we take a broader scope and consider any practices that limit infants’ freedom of movement as restraint. We do not pre-define a list of restraint practices but focus on infants’ real-time opportunities for movement by classifying restraint whenever infants cannot change body position or locomote freely. Quantifying both caregiver restraint (i.e., holding and carrying) and device restraint, we can get a whole picture of the external constraints that infants experience in daily life.

## Previous Measurements of Restraint

Despite the inconsistency in definitions, prior work converges to show that caregiver and device restraints are common in infant daily life, and the time infants spend restrained changes over development. In terms of caregiver restraint, 2- to 6-month-old infants spent around 40% of their awake time being held or carried [@Malachowski2023a; @Siddicky2020f]. Older 10- to 13-month-olds spent around 19% of their awake time being held or carried [@Franchak2024l]. In terms of device restraint, 4- to 6-month-olds spent around 20% of awake time in seating devices, and 35% in placement with physical support [@Callahan1997s; @Malachowski2023a]. Older 10- to 13-month-olds spent around 27% of awake time in restraining furniture or devices [@Franchak2024l]. Additionally, a study that compared across ages also confirmed that infants’ overall restrained time decreased from 2 to 6 months of age [@Carson2022n].

## Restraint Shapes Infant Learning Opportunities

In addition to measuring the total duration of infants’ restrained time, prior research has also examined associations between restraint and developmental outcomes. Motor development appears to be the domain most directly associated with restraint. Various studies have shown that restraint is related to less proficiency in infant motor skills and temporary delay in milestone achievement [@Abbott2001u; @Bartlett2003w; @Carson2022n; @Karasik2023d; @Pin2007z]. Physiological studies tracking infants’ body movements (e.g., spine, legs, hips) in real time suggest that reduced movement opportunities may underlie these effects [@Jiang2016z; @Siddicky2020f; @Siddicky2021c]. Restraint may also influence other aspects of infant development by shaping infants’ learning opportunities in real time. Infants produce fewer object interactions when restrained compared to when they are unrestrained [@Franchak2024l]. Moreover, infants hear less amount of and less consistent adult language when restrained compared to unrestrained [@Malachowski2023a].

Restraint is not always detrimental. Restraint can facilitate certain kinds of learning through stabilizing infants’ body posture or changing their placement in space. For example, restraint devices that support sitting can steady non-sitters’ bodies and facilitate spontaneous object exploration [@Soska2014u]. Moreover, some restraint devices (e.g., high chairs) or carriers put infants in sitting or upright positions high off the ground, in which infants have richer visual input of faces, distant toys, and elevated portions of their environment compared to when prone on the ground [@Kretch2015p; @Kretch2014r; @Franchak2018r; @Luo2020u]. Together, these examples underscore that real-time observation of restraint is essential for understanding how restraint shapes infants’ multimodal learning opportunities from moment to moment.

## Limitations of Previous Research Methods

Researchers have used survey methods, including traditional retrospective report and ecological momentary assessment (EMA), to quantify infants’ restrained time, but survey methods have limitations. Retrospective parent report asks parents to estimate their infant’s average daily restrained time over the past few weeks or month [@Abbott2001u; @Bartlett2003w; @Callahan1997s; @Carson2022n; @Hesketh2015i; @Karasik2018t; @Siddicky2020f]. Despite the widespread use of retrospective reports, they lack precision by relying on parents’ memory—it is difficult to recall and sum every bout of restraint practices and then report an overall time estimation [@Bradburn1987g]. There are exceptions where caregivers were asked to complete structured daily diaries reflecting on the previous day’s restraint practices in short intervals (e.g., every five minutes) [@Karasik2018t; @Majnemer2005j]. However, this method might still be sensitive to memory loss and thus not as accurate in providing data. EMA survey techniques reduce caregivers’ memory burden by making momentary phone-survey prompts to caregivers multiple times (e.g., 5, 10, or 12 times) randomly throughout a day [@Franchak2019o; @Franchak2024l; @Malachowski2023a]. Despite the benefits of gathering immediate observations rather than retrospective reports, EMA cannot provide continuous time-series data. Thus, a method is needed that provides time-sequence information without relying on parent report.

Video recording and human annotation overcomes the above drawbacks of survey-based methods. However, one of the limitations of video observation is being obtrusive. The presence of a researcher in the room may affect the caregiver’s and infant’s behaviors. Another limitation is that video recording and video annotation are both labor-intensive and time-consuming, which limits its use in large-scale or long-duration studies. Indeed, prior work that videotaped and manually annotated infants’ restraint had short durations of recording (e.g., half an hour in @Springfield2025p, and one hour in @Wang2025v). Within the short duration of recording, researchers tend to record the period when infants are awake and active. In contrast, periods when the infants are restrained, like seated in front of TV or in highchairs, would likely be skipped. Thus, video recording is subject to sampling bias and might not be representative of infants’ whole-day experiences.

## Promises and Challenges of Wearable Sensors

A system that uses wearable sensors (i.e., inertial movement units) to record real-time movement data and machine learning models to classify movement categories can overcome the limitations listed above. First, by collecting infants’ movement data through sensors, the system can objectively capture infants’ motion without relying on a respondent or a video annotator’s interpretation. Second, the sensors can provide moment-to-moment observation that is continuous and rich in time-sequence information. Third, sensors embedded in baby garments are mobile and unobtrusive—an experimenter does not need to follow the infant with a camera to record movements. Fourth, the system can record and process full-day or multiple-day data at a time due to the sensors’ sufficient battery life and automated classification of machine learning models.

The past decade has witnessed an uptick of using wearable sensors to record and classify movement categories to enjoy these advantages. The application of wearable sensors progressed from adults [@Arif2015u; @Preece2009n], to children [@Nam2013y; @Ren2016-px; @Stewart2018h], and then to infants [@Airaksinen2020j; @Airaksinen2024z; @Franchak2021k; @Franchak2024e; @Yao2019q]. For infants, there have been established models that classify infants’ body postures in general—including holding and carrying [@Airaksinen2020j; @Franchak2021k; @Franchak2024e], and models that specifically detect holding and carrying periods [@Airaksinen2024z; @Yao2019q]. Insights from prior studies suggest effective practices, such as using multiple sensors and using sensors that record both gyroscope and accelerator data [@Airaksinen2025l; @Franchak2024e].

However, one of the challenges researchers continue to face is the reduced model performance in naturalistic home settings [@Airaksinen2024z; @Franchak2024e; @Yao2019q], compared to more controlled laboratory environments [@Airaksinen2020j; @Franchak2021k]. Indeed, infants may exhibit more complex and unexpected movements at home versus in lab, which requires the models to be able to generalize to scenarios that go beyond the models’ training data set. Moreover, since no prior work has used wearable sensors to detect infants’ overall restrained periods, the current study addresses a unique detection problem.

Detecting restrained periods might be more challenging than classifying body postures. First, infants may show similar body movements whether restrained or not, making restrained and unrestrained classes more difficult to differentiate. For example, infants might kick legs when they are lying without restraint and also when they are strapped in a highchair. Possible distinctions might be manifested in reduced range of motion or repetitive motion due to constraints, which requires high-precision sensors and sensitive models to detect. Second, there are lots of possible restraint forms infants can present—caregiver holding can be holding the infant up straight or holding in arms breastfeeding; seating restraint can happen in countless infant seats, which vary in affordances of moving [@Alghamdi2024d]. Accordingly, the wearable sensor system should be robust enough to generalize across heterogeneous restraint scenarios. Thus, the dual requirement of sensitivity and robustness is the key challenge for developing a reliable model to detect infant restraint in the home.

Strategies to address this untested classification lie in the flexibility of modeling approaches. One strategy is adjusting the window length within which movement data are aggregated. With a fixed sampling rate, larger windows provide more information for the model to leverage to predict the current category. Prior studies have used window lengths ranging from 2.3 to 4 seconds for infant posture category [@Airaksinen2020j; @Franchak2024e], and from 1.15 to 10 seconds for infant holding and carrying detection [@Airaksinen2024z]. Considering that transitions between restraint states are typically slower than transitions between postures, we set window lengths longer than 4 seconds. Theoretically, shorter windows have higher temporal resolution and are sensitive to abrupt changes, but less data might mean the model lacks context to make an accurate classification. In contrast, longer windows can incorporate more data but may lack temporal sensitivity or may introduce irrelevant data from heterogeneous movement within an event. Determining the optimal window length is therefore a key step for the model to reliably detect infants’ restrained periods.

## Current Study

We propose that wearable sensors offer a more objective, unobtrusive, information-rich, and efficient approach to capturing infants’ full-day restraint experiences in natural settings compared to prior methods. To test the approach, we used an existing data set with infants aged from 4 to 14 months (Franchak, 2023). The data set consists of 142 sessions of full-day wearable sensor recording and 1.5-hour video recording of infants’ movement in the home. We coded infants’ restrained status from the videos to provide ground truth labels. We then trained a supervised machine learning model based on the human annotated labels. We varied the window lengths as 4 s, 16 s, and 30 s to examine the impact of window length on model performance.

We validated the model in two ways. First, we compared how accurately model predictions matched human annotation for the 1.5 hours of each video-recorded session. We predicted that accuracy would be good, exceeding \~85% agreement based on prior studies reporting 78—98% accuracy [@Airaksinen2020j; @Airaksinen2024z; @Franchak2024e; @Yao2019q] in classifying infants’ movement at home. We also compared across window lengths according to overall accuracy and prediction bias to decide which window length was most optimal. Second, we applied the model to full-day data to estimate the proportion of time each infant was restrained. We examined the convergent validity of the model against past research by testing whether restrained time decreased with age [@Carson2022n].

# Method

## Dataset

The data set is hosted on Databrary (Franchak, 2023) and has been used in other publications. The data set consists of two cohorts of infants: `r apa_num(age[1,2])` younger infants aged from 4 to 7 months of age, and `r apa_num(age[2,2])` older infants aged from 11 to 14 months of age. Among the total of `r apa_num(age[1,2]+age[2,2])` infants, 26 of them participated in the study for only once, while 39 of them participated from 2 to 4 times, resulting in a total of 142 sessions. Thirty-four of the infants are female and thirty-one are male. Caregivers reported the infants’ race as White (*n* = 23), Hispanic or Latinx (*n* = 8), African American (*n* = 2), Asian (*n* = 2), more than one (*n* = 15), or not fitting into the above races or chose not to answer (*n* = 15).

## Apparatus

In the data set, a special infant garment was customized to secure four lightweight wearable sensors (MC10 Biostamp) at four specific locations (i.e., right thigh, left thigh, right ankle, and left ankle). Each sensor collects raw data as accelerometer signals (in x, y, and z orientations) and gyroscope signals (in roll, pitch, and yaw orientations). The sampling rate of 62.5 Hz allows for high resolution motion recording, and the sufficient battery life and built-in storage allow for whole-day-long recording. An action camera fixed on a mini tripod (GoPro HERO9 Black) was used to record synchronized third-person video of the first 1.5 hours into the session as ground truth.

## Procedure

Data collection took place in the family’s home and began with synchronizing the video camera and wearable sensors. The caregiver was instructed to set up the camera to capture the main room and to move it as needed to ensure the infant remained in view. The caregiver put the garment on the infant and positioned the infant in various postures (including holding and sitting in a restraint device) for at least one minute each for model training purpose. Afterwards, the infant continued wearing the garment during their daily routines. The caregiver logged the times when the infant was napping or when the garment was removed (e.g., for diaper changes, baths, or errands).

## Human Annotation of Restraint

Using the 1.5-hour videos, our team annotated the periods when the infant was restrained or unrestrained using Datavyu software (https://datavyu.org/). Restrained and unrestrained was defined as a binary category, meaning that all times were coded as one of the two categories. A period was coded as missing if the infant went out of view and was excluded from further analysis. Restraint was defined as when infants could not initiate or control their own movements, which included being held or carried by caregivers, or being restrained in devices that constrain them from changing body position. Unrestrained periods, in contrast, were when infants could freely change their body position or locomote. For example, an infant was considered restrained when strapped into a high chair seat, but not when they were placed in a crib and free to change position. Each video file was coded by two coders: a primary coder annotated the whole 1.5-hour video, and an independent reliability coder annotated the first 30 minutes of the video. Inter-rater reliability was calculated as the proportion of video frames where the two coders coded the same label. The human annotation reached a high agreement with an overall accuracy of `r apa_num(stats_human[1,1]*100)`%, ranging from `r apa_num(stats_human[1,2]*100)`% to `r apa_num(stats_human[1,3]*100)`%, and an average kappa of `r apa_num(stats_human[1,4])`, ranging from `r apa_num(stats_human[1,5])` to `r apa_num(stats_human[1,6])`.

## Model Classification of Restraint

The supervised machine-learning classification process followed procedures similar to prior work [@Franchak2021k; @Franchak2024e; @Nam2013y]. First, we aligned the time series of human annotations and sensor data by matching the synchronization time stamp in the video and sensors. This resulted in a data set with human-coded labels and motion data from four sensors—each capturing acceleration (x, y, z) and angular velocity (roll, pitch, yaw)—available at each sampling point.

We aggregated raw IMU data into motion features within sliding windows of *n* seconds, where the window length *n* varied from 4 s to 16 s and 30 s. For each window, we computed statistical features—mean, median, minimum, 25th percentile, 75th percentile, maximum, skewness, kurtosis, standard deviation, and sum—for all six signals across the four sensors, resulting in 240 features. We then added 196 additional features based on cross-sensor and cross-orientation metrics such as correlations and pairwise differences, resulting in 436 features per window.

We started the windowing process from the synchronization point and slid the window every *n*/2 seconds. Consequently, each time point spaced every *n*/2 seconds contains 436 motion features, aggregated from the *n*/2 seconds before and after that point. For model training and testing, we included only time windows where the infant remained in a single status, so that each window could be assigned one unambiguous label.

The third step was to train and validate supervised machine learning models that classify the motion features into restrained status. We applied the random forest algorithm (Breiman, 2001) in modeling using the *DecisionTree* package (Sadeghi et al., 2022) in Julia. We used a leave-one-out cross-validation approach, where we held out one session at each iteration as testing set and trained the model using the other sessions,to evaluate how well the model generalized to unseen data. After validation, an optimal window length was picked[,]{.underline} and a final model was trained using all the human annotations to process whole-day sensor data of all sessions.

## Data Sharing

Machine-learning process was conducted in Julia version 1.11.1 using the DecisionTree package (Sadeghi et al., 2022). Model evaluation, statistical analyses, and this reproducible manuscript were conducted in RStudio environment (R version 4.5.0; RStudio version 2025.5.0.496; RStudioTeam, 2024) using the R packages tidyverse [@tidyverse], flextable [@flextable], and papaja [@papaja], as well as the apa quarto template (Schneider, n.d.). All human annotation results and processing scripts are publicly shared on GitHub (XXX).

# Results

We first compared the model’s performance to the human annotation of the 1.5-hour video recorded period across all 142 sessions using leave-one-out cross-validation approach. We reported performance metrics including accuracy, kappa, sensitivity, positive predictive value, F1, and the correlation of overall estimate time of restraint between the model and human annotation. We also examined prediction biases model and how they vary across window lengths. Second, we applied the model with the optimal window length to process infants’ full-day sensor data and examined the model’s convergent validity by replicating a previously-reported effect: that restraint time decreases with age [@Carson2022n].

## Model Prediction Compared to Human Annotation

```{r}
load("matrices.RData")
matrix_comb <- rbind(matrix_30, matrix_16, matrix_4)
matrix_comb <- matrix_comb %>%  
  clean_names() %>% 
  mutate(model = factor(model, levels=c("4_no_pos","16_no_pos","30_no_pos")),
         positive_class = factor(positive_class, levels=c('r','u'))) %>% 
  mutate(prevalence_df = detection_prevalence - prevalence)

```

```{r tbl-indexbetweenmodels}
#| tbl-cap: Performance metrics according to window length
#| ft.align: left
#| apa-note: F, df, and p value indicate one-way ANOVA on each metric comparing across the three window lengths.

# Compare between models
# Descriptive
table_mean <- matrix_comb %>%
  group_by(positive_class, model) %>%
  summarise(
    mean_acc = mean(accuracy, na.rm = TRUE),
    mean_kappa = mean(kappa, na.rm = TRUE),
    mean_sen = mean(sensitivity, na.rm = TRUE),
    mean_ppv = mean(pos_pred_value, na.rm = TRUE),
    mean_f1 = mean(f1, na.rm = TRUE),
    .groups="drop"
  ) %>%
  pivot_longer(mean_acc:mean_f1, names_to = "Metric", values_to = "mean") %>%
  pivot_wider(names_from = model, values_from = mean) %>% 
  mutate(Metric = recode(Metric,
                        "mean_acc" = "Accuracy",
                        "mean_kappa" = "Kappa",
                        "mean_sen" = "Sensitivity",
                        "mean_ppv" = "PPV",
                        "mean_f1" = "F1"),
         positive_class = recode (positive_class,
                                  "r" = "Restrained",
                                  "u" = "Unrestrained")) %>%
  relocate("Metric", .before = "positive_class") %>% 
  mutate(Metric = factor(Metric, levels=c("Accuracy", 
                                            "Kappa",
                                            "Sensitivity", 
                                            "PPV", 
                                            "F1"))) %>% 
  arrange(Metric) %>% 
  rename("Category" = "positive_class",
         "4 s" = "4_no_pos",
         "16 s" = "16_no_pos",
         "30 s" = "30_no_pos")

# Between models: ANOVA
aov_mdl <- function(index, ctgr){
  temp_ds <- matrix_comb %>% filter(positive_class==ctgr)
  fmla <- reformulate("model", response = index)  # response ~ model
  anova <- tidy(aov(fmla, data = temp_ds))
}
aov_r <- map(indx, ~aov_mdl(index=.x, ctgr="r")) %>% 
  map2_dfr(indx, ~mutate(as.data.frame(.x), Metric = .y)) %>% 
  mutate(Category = "r")
aov_u <- map(indx, ~aov_mdl(index=.x, ctgr="u")) %>% 
  map2_dfr(indx, ~mutate(as.data.frame(.x), Metric = .y)) %>% 
  mutate(Category = "u")
table_aov <- rbind(aov_r, aov_u) %>% 
  mutate(df_resi = lead(df, n = 1)) %>% 
  filter(term=="model") %>% 
  mutate(df = str_glue("({df}, {df_resi})")) %>% 
  select(-term, -df_resi, -sumsq, -meansq) %>% 
  rename("F" = "statistic",
         "p" = "p.value") %>% 
  relocate("Metric", .before = "df") %>% 
  relocate("Category", .before = "df") %>% 
  relocate("df", .before = "p") %>%
  mutate(Metric = recode(Metric,
                        "accuracy" = "Accuracy",
                        "kappa" = "Kappa",
                        "sensitivity" = "Sensitivity",
                        "pos_pred_value" = "PPV",
                        "f1" = "F1")) %>%
  mutate(Metric = factor(Metric, levels=c("Accuracy", 
                                            "Kappa",
                                            "Sensitivity", 
                                            "PPV", 
                                            "F1"))) %>% 
  arrange(Metric)

# Combine descriptive and ANOVA results and put them in a flex table
table_btw_mdl <- cbind(table_mean, table_aov %>% select(-Metric, -Category)) %>% 
  slice(-2,-4) %>% 
  mutate(p = ifelse(p < .001,"<.001",round(p,3)),
         p = str_remove(p, "^0")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3)))

table_btw_mdl[1,2] <- ""
table_btw_mdl[2,2] <- ""

table_flex_btw_mdl <- table_btw_mdl %>% 
  flextable() %>% flextable::theme_apa() %>% line_spacing(part = "all") %>% 
  merge_v(j = "Metric") %>% fix_border_issues() %>% 
  hline(i = c(1,2,4,6,8)) %>% 
  padding(padding.top = 2, padding.bottom = 2) %>% 
  set_table_properties(layout = "autofit", width = .99) %>% 
  align(align = "center", part = "all")
table_flex_btw_mdl
```

```{r}
# For per-class metrics (i.e., sens, ppv, f1), calculate the CI of the metric difference btw classes (i.e., r and u)
# Don't need an APA table for this, but need the table to call values from
# Function to bootstrap CI of gap for each model and each metric
bootstrap_gap <- function(df, metric, B = 2000, gap_cal = F) {
  if (gap_cal == F){
    df_wide <- df %>%
    select(session, id, positive_class, !!sym(metric)) %>%
    pivot_wider(names_from = positive_class, values_from = !!sym(metric)) %>%
    mutate(gap = r - u,
           id_session = paste(id, session, sep = "_"))
  } else{
    df_wide <- df %>% 
      mutate(id_session = paste(id, session, sep = "_"))
  } # if gap column is calculated or not
  
  sessions <- unique(df_wide$id_session)
  n <- length(sessions)
  res <- numeric(B)
  
  for (b in seq_len(B)) {
    sampled <- sample(sessions, size = n, replace = TRUE)
    df_b <- df_wide %>% filter(id_session %in% sampled)
    res[b] <- mean(df_b$gap, na.rm = TRUE)
  }
  
  ci <- quantile(res, c(0.025, 0.975), na.rm = TRUE)
  
  list(
    metric = metric,
    mean_gap = mean(df_wide$gap, na.rm = TRUE),
    ci = ci
  )
}

table_btw_ctg <- crossing(model = model_labels, metric = c("sensitivity","pos_pred_value")) %>%   # all combinations
  mutate(result = map2(model, metric, 
                       ~ bootstrap_gap(df = matrix_comb %>% filter(model == .x), 
                                       metric = .y))) %>%
  mutate(result = map(result, ~ tibble(
    mean_gap = .x$mean_gap,
    ci_low   = .x$ci[1],
    ci_high  = .x$ci[2]
  ))) %>% 
  unnest(result) %>% 
  mutate(model = factor(model, levels = c("4_no_pos","16_no_pos","30_no_pos")),
         metric = factor(metric, levels = c("sensitivity","pos_pred_value"))) %>% 
  mutate(model = recode(model,
                        "4_no_pos" = "4 s",
                        "16_no_pos" = "16 s",
                        "30_no_pos" = "30 s"),
         metric = recode(metric,
                         "sensitivity" = "Sensitivity",
                         "pos_pred_value" = "PPV")) %>% 
  arrange(model, metric) %>% 
  rename("Window Length" = "model",
         "Metric" = "metric",
         "M"="mean_gap",
         "95%CI lower bound"="ci_low",
         "95%CI upper bound"="ci_high")
# 
# table_flex_btw_ctg <- table_btw_ctg %>% 
#   flextable() %>% flextable::theme_apa() %>% 
#   line_spacing(part = "all") %>%
#   merge_v(j = "Window Length") %>% fix_border_issues() %>% 
#   add_header_row(values = c("","", "Difference (rest.- unrest.)"), colwidths = c(1,1,3)) %>% 
#   hline(i = c(2,4,6), part = "body") %>%
#   padding(padding.top = 2, padding.bottom = 2) %>% 
#   set_table_properties(layout = "autofit", width = .99) %>% 
#   align(align = "center", part = "all")
# table_flex_btw_ctg
```

```{r fig-cibtwctg}
#| fig-cap: Differences in sensitivity and PPV between restrained and unrestrained prediction according to window length
#| fig-width: 8
#| fig-height: 5
#| apa-note: Each vertical line indicates the 95%CI of the metric's difference between restrained prediction and unrestrained prediction. The dot on each line denotes the mean of the difference (restrained - unrestrained).

p_ci_sen <- ggplot (table_btw_ctg %>% 
                      filter(Metric=="Sensitivity"),
                    aes(x=`Window Length`, 
                        y = `M`,
                        ymin = `95%CI lower bound`, 
                        ymax = `95%CI upper bound`, 
                        color = `Window Length`))+
  geom_pointrange(position = position_dodge(width = 0.5), size = 0.7) +
  geom_segment(aes(x = 0.5, xend = 4, y = 0, yend = 0),
             linetype = "dashed", color = "black")+
  scale_color_manual(values=model_colors,labels=c("4 s","16 s", "30 s")) +
  scale_y_continuous(
    limits = c(-0.15, 0.15),
    breaks = c(-0.15,-0.1, -0.05, 0, 0.05, 0.1, 0.15)
  )+
  labs(x = "",y = "Metric value difference (rest. - unrest.)")+
  guides(color="none")+
  ggtitle("A. Sensitivity value difference")+
  annotate("text", x = 3.8, y = -0.12, label="More sensitive \n to unrestrained",color = "black", hjust=1, size = 4.5)+
  annotate("segment",
         x = 3.9, xend = 3.9,  # arrow start → end
         y = -0.1,  yend = -0.14,
         arrow = arrow(length = unit(0.2, "cm")),
         size = 1, color = "black")+
  annotate("text", x = 3.8, y = 0.12, label="More sensitive \n to restrained",color = "black", hjust=1, size = 4.5)+
  annotate("segment",
           x = 3.9, xend = 3.9,  # arrow start → end
           y = 0.1,  yend = 0.14,
           arrow = arrow(length = unit(0.2, "cm")),
           size = 1, color = "black")+
  theme_update()

p_ci_ppv <-ggplot (table_btw_ctg %>% 
                      filter(Metric=="PPV"),
                    aes(x=`Window Length`, 
                        y = `M`,
                        ymin = `95%CI lower bound`, 
                        ymax = `95%CI upper bound`, 
                        color = `Window Length`))+
  geom_pointrange(position = position_dodge(width = 0.5), size = 0.7) +
  geom_segment(aes(x = 0.5, xend = 4, y = 0, yend = 0),
             linetype = "dashed", color = "black")+
  scale_color_manual(values=model_colors,labels=c("4 s","16 s", "30 s")) +
  scale_y_continuous(
    limits = c(-0.15, 0.15),
    breaks = c(-0.15,-0.1, -0.05, 0, 0.05, 0.1, 0.15)
  )+
  labs(x = "",y = "")+
  guides(color="none")+
  ggtitle("B. PPV value difference")+
  annotate("text", x = 3.8, y = -0.12, label="More credible \n unrestrained predictions",color = "black", hjust=1, size = 4.5)+
  annotate("segment",
         x = 3.9, xend = 3.9,  # arrow start → end
         y = -0.1,  yend = -0.14,
         arrow = arrow(length = unit(0.2, "cm")),
         size = 1, color = "black")+
  annotate("text", x = 3.8, y = 0.12, label="More credible \n restrained predictions",color = "black", hjust=1, size = 4.5)+
  annotate("segment",
           x = 3.9, xend = 3.9,  # arrow start → end
           y = 0.1,  yend = 0.14,
           arrow = arrow(length = unit(0.2, "cm")),
           size = 1, color = "black")+
  theme_update()

(p_ci_sen + p_ci_ppv)
```

```{r}
# Table. Model predicted and human annotated prevalence of restrained periods, their correlation, and differences
df_prev <- matrix_comb %>%
  filter(positive_class=="r") %>%
  select(model, id, session, detection_prevalence, prevalence) %>% 
  mutate(gap = detection_prevalence-prevalence)

prev_gap <- map(model_labels, ~ bootstrap_gap(df = df_prev %>% filter(model == .x), metric = "gap", gap_cal = T))

table_prev <- df_prev %>%
  group_by(model) %>%
  summarise(actual_prev = mean(prevalence),
            prev = mean(detection_prevalence),
            cor = cor (detection_prevalence, prevalence),
            .groups = "drop") %>% 
  mutate(gap = map(prev_gap, ~ tibble(
    mean_gap = .x$mean_gap,
    ci_low   = .x$ci[1],
    ci_high  = .x$ci[2]))) %>%
  unnest(gap) %>% 
  mutate(model=recode(model,
                      "4_no_pos" = "4 s",
                       "16_no_pos" = "16 s",
                       "30_no_pos" = "30 s"))

# table_prev_100 <- table_prev %>% 
#   mutate(actual_prev=actual_prev*100,
#          prev = prev*100, 
#          mean_gap = mean_gap *100,
#          ci_low=ci_low * 100,
#          ci_high=ci_high*100) %>% 
#   mutate(ci = paste0("[",round(ci_low, 2),", ", round(ci_high, 2), "]" )) %>% 
#   select(-ci_low, -ci_high, -actual_prev, -prev) %>% 
#   rename("Window Length" = "model",
#          "Correlation (r)" = "cor",
#          "M" = "mean_gap",
#          "95%CI" = "ci")
# 
# table_flex_prev <- table_prev_100 %>% 
#   flextable() %>% flextable::theme_apa() %>% line_spacing(part = "all") %>% 
#   fix_border_issues() %>% 
#   add_header_row(values = c("","Prediction Error (%)"), colwidths = c(2,2)) %>% 
#   hline(i = c(1,2,3)) %>% 
#   padding(padding.top = 2, padding.bottom = 2) %>% 
#   set_table_properties(layout = "autofit", width = .99) %>% 
#   align(align = "center", part = "all")
# table_flex_prev
```

```{r fig-prevalencegap}
#| fig-cap: Restraint prevalence 
#| fig-width: 8
#| fig-height: 4
#| apa-note: Prevalence is defined as the proportion of windows predicted (or annotated) as restrained among all windows. (A) shows the correlation of prevalence between model prediction and human annotation. The black dashed identity line indicates perfect agreement. Each circular symbol represents one session for a specific window length (denoted by color). (B) shows the difference of prevalence between human annotation and model prediction (i.e., prediction error). A negative prediction error indicates the model estimated less amount of time infants were restrained compared to human annotation. The horizontal line in the box denotes the median; the boarder lines of the box denotes the first (Q1) quartile and the third (Q3) quartile; and the lines extended denotes the min and max data points within the range of 1.5 IQR (i.e., Q3-Q1).

p_prev_cor <- ggplot(data=matrix_comb %>% filter(positive_class=="r") %>%
                   mutate(model=recode(model,
                                       "4_no_pos" = "4 s",
                                       "16_no_pos" = "16 s",
                                       "30_no_pos" = "30 s")),
                 aes(x=prevalence, y=detection_prevalence,
                     color=model))+
  geom_point(shape = 16,size = 3, alpha=.3) + 
  geom_abline(intercept = 0, slope = 1, linetype="dashed", color = "black") +
  geom_smooth(method = "lm", se = FALSE) +  
  geom_text(data = table_prev,
            aes(x = c(1.16,1.16,1.16),
                y = c(.7,.8,.9),
                label = paste0("r = ", round(cor,2)),
                color = model, size = 12), 
            show.legend = FALSE) +
  scale_color_manual(values=model_colors, labels=c("4 s", "16 s","30 s")) +
  scale_x_continuous(limits = c(0,1.2), breaks=c(0,.5,1)) +
  scale_y_continuous(limits = c(0,1), breaks=c(0,.5,1))+
  coord_cartesian(clip = "off")+
  labs(x = "Human annotated prevalence", y = "Model predicted prevalence")+
  theme_update()+
  theme(axis.title.x = element_text(hjust = 0.37))+
  guides(color = "none")+
  ggtitle("A. Prevalence Correlation")

p_prev_gap <- ggplot(data=matrix_comb %>% filter(positive_class=="r") %>%
                   mutate(model=recode(model,
                                       "4_no_pos" = "4 s",
                                       "16_no_pos" = "16 s",
                                       "30_no_pos" = "30 s")),
                 aes(x=model, y=(detection_prevalence-prevalence)*100, color=model))+
  geom_boxplot(width = 0.1, outlier.shape = NA, fill = "white") + 
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  scale_color_manual(values=model_colors, labels=c("4 s", "16 s","30 s")) +
  scale_y_continuous(
    limits = c(-20,20),
    breaks = c(-20, -10, 0, 10, 20)
  )+
  labs(x="Window Length", y="Prediction Error (%)", color="Window Length")+
  theme_update()+
  theme(legend.position = c(0.5,0.9),legend.direction = "horizontal")+
  ggtitle("B. Prevalence Offset")

(p_prev_cor+p_prev_gap)

```

```{r}
#| eval: false
#| include: false
# check if the data set is imbalanced 
matrix_comb %>% 
  filter(positive_class=="r") %>% 
  group_by(model) %>% 
  summarize(mean_prev = mean(prevalence),
            sd_prev = sd(prevalence),
            .groups="drop")


ggplot(matrix_comb %>% filter(positive_class=="r"), aes(x = prevalence, color=model)) +
  geom_density(alpha = 0.4)

#slightly imbalanced: less unrestrained periods, the ratio (~40%) is similar to the total estimated proportion
```

```{r}
#| eval: false
#| include: false
# check accuracy outliers 
outlier_4 <- matrix_comb %>%
  filter(model == "4_no_pos", positive_class=="r") %>%
  mutate(
    q1 = quantile(accuracy, 0.25),
    q3 = quantile(accuracy, 0.75),
    iqr = q3 - q1,
    is_outlier = accuracy < (q1 - 1.5 * iqr) | accuracy > (q3 + 1.5 * iqr)
  ) %>%  filter(is_outlier)
outlier_16 <- matrix_comb %>%
  filter(model == "16_no_pos", positive_class=="r") %>%
  mutate(
    q1 = quantile(accuracy, 0.25),
    q3 = quantile(accuracy, 0.75),
    iqr = q3 - q1,
    is_outlier = accuracy < (q1 - 1.5 * iqr) | accuracy > (q3 + 1.5 * iqr)
  ) %>%  filter(is_outlier)
outlier_30 <- matrix_comb %>%
  filter(model == "30_no_pos", positive_class=="r") %>%
  mutate(
    q1 = quantile(accuracy, 0.25),
    q3 = quantile(accuracy, 0.75),
    iqr = q3 - q1,
    is_outlier = accuracy < (q1 - 1.5 * iqr) | accuracy > (q3 + 1.5 * iqr)
  ) %>%  filter(is_outlier)

outliers <- rbind(outlier_4,outlier_16, outlier_30)

ggplot(data=outliers, aes(x=session, y=id, color=model))+
  geom_jitter()+
  geom_text(aes(label = paste0("(", id, ", ", session, ")")),
            vjust = -0.5,   # move label a bit above point
            size = 3) 
```

### Overall Model Performance

@tbl-indexbetweenmodels shows that all window lengths reached high accuracy (`r apa_num(table_btw_mdl[1,3])` for 4 s, `r apa_num(table_btw_mdl[1,4])` for 16 s, and `r apa_num(table_btw_mdl[1,5])` for 30 s), and good kappa values (`r apa_num(table_btw_mdl[2,3])` for 4 s, `r apa_num(table_btw_mdl[2,4])` for 16 s, and `r apa_num(table_btw_mdl[2,5])` for 30 s). All window lengths presented high sensitivity—how often the model correctly detected when the infant was actually restrained or unrestrained—ranging from `r apa_num(min(table_btw_mdl[3,3:5]))` to `r apa_num(max(table_btw_mdl[3,3:5]))` for restrained prediction, and `r apa_num(min(table_btw_mdl[4,3:5]))` to `r apa_num(max(table_btw_mdl[4,3:5]))` for unrestrained prediction. All window lengths had high positive predictive value (PPV)—how often the model’s predictions of being restrained or unrestrained were correct—ranging from `r apa_num(min(table_btw_mdl[5,3:5]))` to `r apa_num(max(table_btw_mdl[5,3:5]))` for restrained prediction, and `r apa_num(min(table_btw_mdl[6,3:5]))` to `r apa_num(max(table_btw_mdl[6,3:5]))` for unrestrained prediction. All window lengths also had high F1 scores—a balanced measure that combines sensitivity and PPV—ranging from `r apa_num(min(table_btw_mdl[7,3:5]))` to `r apa_num(max(table_btw_mdl[7,3:5]))` for restrained prediction, and `r apa_num(min(table_btw_mdl[8,3:5]))` to `r apa_num(max(table_btw_mdl[8,3:5]))` for unrestrained prediction. Moreover, the three window lengths did not significantly differ for any of the metrics (*p*s \>= `r apa_num(min(table_btw_mdl[1:8,8]))`).

@fig-prevalencegap A shows that, at session level, all window lengths provided reliable estimate of restrained time. We defined prevalence as the proportion of windows classified as restrained among all windows for each session. The prevalence predicted by the model was highly correlated with the prevalence annotated by human coders (*r* = `r apa_num(table_prev[1,4])` for 4 s, *r* = `r apa_num(table_prev[2,4])` for 16 s, and *r* = `r apa_num(table_prev[3,4])` for 30 s).

### Models were biased towards predicted unrestrained

Despite the overall good accuracy, models using all three window lengths were biased towards predicted that infants were unrestrained rather than restrained. @fig-cibtwctg shows the difference of a metric’s value (e.g., sensitivity, PPV) between restrained class and unrestrained class within a single window length. Larger difference scores (away from 0) indicate larger asymmetry in predicting one category rather than the other. @fig-cibtwctg A shows that the sensitivity difference for 4 s window was `r apa_num(table_btw_ctg[1,3])`, 95% bootstrapped CI \[`r apa_num(table_btw_ctg[1,4])`, `r apa_num(table_btw_ctg[1,5])`\], for 16 s window was `r apa_num(table_btw_ctg[3,3])`, 95% CI \[`r apa_num(table_btw_ctg[3,4])`, `r apa_num(table_btw_ctg[3,5])`\], and for 30 s window was `r apa_num(table_btw_ctg[5,3])`, 95% CI \[`r apa_num(table_btw_ctg[5,4])`, `r apa_num(table_btw_ctg[5,5])`\]. The negative differences of sensitivity across all window lengths indicate that the model was more sensitive in detecting the unrestrained class compared to restrained class regardless of window lengths. @fig-cibtwctg B shows that the PPV difference for 4 s window was `r apa_num(table_btw_ctg[2,3])`, 95% CI \[`r apa_num(table_btw_ctg[2,4])`, `r apa_num(table_btw_ctg[2,5])`\], for 16 s window was `r apa_num(table_btw_ctg[4,3])`, 95% CI \[`r apa_num(table_btw_ctg[4,4])`, `r apa_num(table_btw_ctg[4,5])`\], and for 30 s window was `r apa_num(table_btw_ctg[6,3])`, 95% CI \[`r apa_num(table_btw_ctg[6,4])`, `r apa_num(table_btw_ctg[6,5])`\]. The positive differences of PPV across all window lengths indicate that the model’s prediction of restrained periods was more credible compared to its prediction of unrestrained periods. Notably, while none of the window lengths were free from bias, the 30 s window presented smaller offsets of metric values between restrained and unrestrained detection.

The prediction bias was also evident when examining the model’s overall estimate of restrained time and its difference from human annotation. @fig-prevalencegap B shows that the model predictions consistently underestimated overall restrained time compared to human annotation, despite the high correlation between the two. We defined prediction error as the model predicted prevalence (in % units) subtracted by the human annotated prevalence of restrained periods (in % units). The 4 s window had a prediction error of `r apa_num(table_prev[1,5]*100)`%, 95% CI \[`r apa_num(table_prev[1,6]*100)`%, `r apa_num(table_prev[1,7]*100)`%\], the 16 s window of `r apa_num(table_prev[2,5]*100)`%, 95% CI \[`r apa_num(table_prev[2,6]*100)`%, `r apa_num(table_prev[2,7]*100)`%\], and the 30 s window of `r apa_num(table_prev[3,5]*100)`%, 95% CI \[`r apa_num(table_prev[3,6]*100)`%, `r apa_num(table_prev[3,7]*100)`%\].

Across the metrics of sensitivity, PPV, and prediction error, 30 s turned out to be the best window length out of the three, which brought about a balanced performance across classes.

## Model Estimate Compared to Previous Studies

```{r}
# exclude sessions where total recording time is <3 hours
ds_excl <- ds_sum %>% filter(total_time < 3) #11 sessions 
ds_sum_excl <- ds_sum %>% filter(total_time >=3) #132 sessions left
ds_sum_excl <- ds_sum_excl %>% 
  filter(unique_id != "148/1",
         unique_id != "178/3")

stats_fullday <- ds_sum_excl %>% 
  group_by(age_group) %>% 
  summarize(mean_restrained = mean(restrained_prop),
            sd_restrained = sd(restrained_prop),
            .groups="drop")

```

```{r fig-agechange}
#| fig-cap: Younger versus older infants’ full day restrained time (as a proportion of recording time) 
#| fig-width: 5
#| fig-height: 4
#| apa-note: Each point denotes one session. The horizontal red line denotes the group mean of restrained time proportion.

t_age <- summary(lm(restrained_prop ~ age_group, data = ds_sum_excl))$coefficients # t test

p_age <- ggplot(ds_sum_excl, aes(x=age_group, y=restrained_prop)) +
  geom_jitter(width=.2, shape = 1,size = 3, color="black") + 
  stat_summary(fun = mean, geom = "crossbar",
               width = 0.4, color = "red") +
  scale_y_continuous(name = "Restrained time proportion of awake time", breaks = seq(0, 1, .25), limits = c(0,1))+
  theme_update()+
  labs(x="Age Group")

p_age

```

We applied the best model based on the analyses in the previous (30-s window length) to create full-day predictions for all sessions that included full-day experiences. We excluded eleven sessions whose total recording time was less than 3 hours, which were useful for training and validating the models but insufficient for measuring day-long experience. We also excluded another two sessions where the family took the garment off for a substantial portion of time but did not log the time stamp of removal. The remaining 129 sessions had an average recording time of `r apa_num(mean(ds_sum_excl$total_time))` hours (*SD* = `r apa_num(sd(ds_sum_excl$total_time))`).

Model predicted restrained and unrestrained time converged with previous studies [@Carson2022n]. Younger infants aged from 4 to 7 months spent *M* = `r apa_num(stats_fullday[1,2]*100)`% (*SD* = `r apa_num(stats_fullday[ 1,3]*100)`%) of awake time restrained, and older infants aged from 11 to 14 months spent *M* = `r apa_num(stats_fullday[2,2]*100)`% (*SD* = `r apa_num(stats_fullday[2,3]*100)`%) of awake time restrained. Moreover, @fig-agechange shows a significant group difference in restrained time between older infants and younger infants (*t* = `r apa_num(t_age[2,3])`, *p* `r apa_num(ifelse(t_age[2,4] < .001, "<.001", t_age[2,4]))`), supporting the convergent validity of the current system.

# Discussion

In the current study, we developed and validated a machine learning algorithm to detect infants’ restrained experiences from wearable sensors during home life. The model presented high accuracy and substantial kappa agreement compared to human-annotated ground truth, which are comparable to other wearable sensor algorithms classifying other infant movement categories in the home [@Airaksinen2020j; @Airaksinen2024z; @Franchak2024e; @Yao2019q]. The model presented a slight bias towards predicted unrestrained—the model tended to overestimate unrestrained periods but was not as credible when labeling a period as unrestrained. The bias might be due to the inherent imbalance in the training data set where unrestrained periods were more prevalent than restrained periods, indicated by human annotated prevalence. However, using a longer window length (i.e., 30 s, rather than 16 s or 4 s) mitigated the prediction bias. Ultimately, the model with 30 s window length presented convergent validity in overall estimate of restraint time and age-related change compared with previous findings [@Carson2022n].

Why could a longer window length (i.e., 30 s) provide more balanced detection? For one thing, a longer window length can better capture the contextual information of restraint. Consider the contextual nature of restraint: infants may exhibit similar body movement whether restrained or not for most of the time, but one subtle occurrence of distinctive movement can signal restraint. For example, an infant can be sitting on the floor unrestrained, on caregiver’s lap restrained, or strapped in a seat restrained. It is hard to distinguish if the infant is restrained while sitting still, but when the infant moves their limbs, restraint may present differences, such as a stronger damping in the motion decay because the support (i.e., caregiver’s laps, or seats) absorbs the motion. These differences only present themselves in the moments of movement, not throughout the whole period. Thus, a longer window length can better capture temporal information, whereas shorter windows only capture momentary movements that may appear similar across restrained and unrestrained periods. For another, a longer window length can improve generalization across a variety of restraint forms. Infants can be restrained in various ways—such as in stationary seats, in jumpers, in swings, or in caregivers’ arms, either still or while being carried around. Different restraint forms are presented by different motion features. By encompassing a broader range of movement patterns, a longer window can provide more informative and generalizable features for the model to account for variability within the restrained class. Given a restrained bout usually lasts minutes rather than seconds, a 30 s window length presents a good balance.

Our analysis of window length informs future modeling works to consider temporal resolution as a behavior-informed decision. In infants’ everyday activities, behaviors unfold across various timescales—from second-by-second movement (e.g., locomotion) to longer periods of states (e.g., on the ground vs. lifted). Our optimal window length for restraint detection turns out to be longer than the window lengths in other models for body position classification [@Airaksinen2020j; @Franchak2021k; @Franchak2024e; @Yao2019q], corresponding to the fact that restraint bouts last longer than position bouts. We recommend selecting window lengths that align with typical duration and transition of the targeted behavior for machine learning modeling.

## Advances in Methodology

The key contribution of this work is introducing a novel approach for measuring infant restraint in the home environment using wearable sensors. Wearable sensors offer several advantages over previous methods when quantifying day-long movement in naturalistic settings. Compared with survey methods, wearable sensors provide objective, continuous measurements with fine-grained temporal resolution. Compared with video recording and human coding method, wearable sensors are less obtrusive and enable substantially longer recordings. With the machine learning model established, researchers can extend their investigation timescales to a full day or even multiple days at low cost. Although the current machine learning model achieves slightly lower accuracy (89%) than human annotation, the trade-off between accuracy and human labor favors our sensor approach—especially when the target behavior fluctuates over longer timescales beyond the practical limits of human coding. Restraint is such a behavior that does not recur repeatedly every hour but follows the rhythm of everyday routines and activities. Therefore, a wearable sensor system capable of generating restraint profiles across full days offers greater scalability for measuring infants’ everyday experiences.

The current work also demonstrates that wearable sensors have the potential to characterize other categories of infant movement that might interest researchers in a variety of domains. Compared with previous works that classify infants’ positions [@Airaksinen2020j; @Airaksinen2024z; @Franchak2021k; @Franchak2024e; @Yao2019q], restraint might be the most challenging one out of all categories, because the motion between classes can be very similar sometimes—such as sitting on the ground versus sitting on a caregiver’s lap. In contrast, other categories (e.g., positions) usually have distinctive motion features between classes—such as the legs’ moving direction in supine position versus upright position. Our results, however, showed that even a context-dependent category as restraint can be successfully classified by machine learning models using sensor data. Conceivably, this system might perform comparably well in classifying other context-independent categories, such as locomotion, steps, falling, etc. Future research needs to further develop the potential of wearable sensors. If proven feasible, the new models will substantially reduce the need for human labor and enlarge the data scale.

Beyond expanding the range of classifiable behaviors, future works should also broaden the scenarios in which wearable sensors are applied. To date, most applications of wearable sensors have been limited to laboratory or home environments. Infants’ outdoor experiences remain largely unexplored, whereas outdoor environments provide infants with rich and unique opportunities for learning and exploration. How much time infants spend restrained in stroller or car seat? How positions are they frequently in? How many steps they take? Validating sensor-based models in outdoor scenarios can open new avenues for investigation.

Moving forward, future works can also integrate different categories of model predictions to generate a multifaceted description of infant real-time experience. For example, by combing the model prediction of infant positions (i.e., supine, prone, sitting, upright, holding and carrying) with the current model prediction of overall restraint, researchers can easily know what positions infants were in while restrained—such as supine restrained, sitting restrained, or being held—without spending extra efforts in human coding and model training.

## Advances in Understanding Full-day Real-time Restraint

The advances in methodology can facilitate a deeper understanding of infant full-day real-time restraint experiences. Restraint, as an external factor imposed by caregivers, plays a critical role in infant real-time experiences—influencing when and how often infants have the opportunities for movement, which portions of the environment they can perceive, and what objects and people they can interact with. Prior works provided valuable insights into parental attitudes of restraint and physical activity. For example, parental concerns over floor play, beliefs about physical activeness, and practices of co-play are associated with restraint frequency and infants’ physical activity level [@Hanisch2025j; @Hnatiuk2013t; @Prioreschi2017c; @Hesketh2015i; @Karasik2025h]. However, few studies have examined how each restraint decision is formed in real time and how infants’ sensorimotor experiences are affected in real time.

Having a full-day continuous profile of infant restraint is thus essential in identifying the factors influencing restraint decisions in real time. Restraint is believed to serve practical purposes such as safety, transportation, comforting, etc. [@Birken2015s], but these functions alone cannot account for all the variability in when caregivers choose to restrain an infant. To what extent are such decisions influenced by these intra-individual factors, such as daily routines, momentary tasks, or fluctuations in emotional states? To what extent are such decisions influenced by inter-individual factors, such as infant motor skill, attachment style, caregivers’ beliefs, or parental stress? And to what extent are the decisions influenced by socio-cultural factors, such as cultural norms [@Karasik2015i; @Karasik2018t], or socioeconomic status? The current system’s capability of locating each individual bout of restraint enables more fine-grained analyses of caregiver behavior and provides new insights into the dynamics of everyday caregiving practices.

Having a full-day continuous profile of infant restraint is also essential in understanding infant real-time sensorimotor experiences. An overall estimate of restraint time is not enough to examine the mechanisms between restraint practices and development outcomes. Take motor development as an example. @Karasik2023d found that Tajik infants fell behind US infants in achieving motor milestones despite the comparable overall restrained time between cultural groups, might due to the prolonged individual restrained bouts in Tajik infants [@Karasik2022a]. The current system allows researchers to characterize the temporal structure of restraint and examine the relationship between restraint and other experiences. A prior study [@Malachowski2023a] found device restraint is associated with reduced adult language input using ecological momentary assessment. However, a continuous, full-day recording of restraint captures the temporal distribution rather than snapshots. Questions can be answered such as: Does adult word count vary based on the duration of each restraint bout? How do specific language inputs (e.g., referential words) distribute throughout a day based on restraint states and restraint types? Additionally, real-time restraint can also be linked to infants’ self-initiated explorations. For example, how do infants’ own vocalizations, attention, locomotion, or object interactions differ across restrained and unrestrained? In short, continuous, full-day quantification of infant restraint offers a foundation for linking caregiver practices to infants’ moment-to-moment sensorimotor experiences, thereby deepening our understanding of how daily contexts accumulate shape developmental outcomes.

## Conclusions

To conclude, the current study provided a powerful tool for quantifying infants’ everyday restraint experiences in the home using wearable sensors and machine learning modeling. The model achieved high accuracy and strong agreement with human annotations, as well as convergent validity by showing an age-related decrease in infant restraint time. We explored the window length in the data aggregating procedure of modeling and proved that a longer window enhanced the model’s ability to capture the contextual heterogeneous nature of restraint. Together, the current study advanced the method for measuring infant full-day, real-time movement, and opened avenues for research on how external constraints from caregivers shape early development.

# References
