---
title: "Quantifying Infants' Everyday Restrained Experiences in the Home Using Wearable Inertial Sensors"
blank-lines-above-title: 2
shorttitle: "Quantify infant restrained time with sensors"
fig-dpi: 300
execute: 
  echo: false
  warning: false
  error: false
  cache: false
author:
  - name: Hanzhi Wang
    orcid: 0009-0000-0520-5935
    email: hanzhi.wang@email.ucr.edu
    affiliations:
      - id: id1
        name: University of California, Riverside
        department: Department of Psychology
  - name: Hailey N. Rousey
    orcid: XX
    email: XX
    affiliations:
      - id: id1
        name: University of California, Riverside
        department: Department of Psychology
  - name: John M. Franchak
    corresponding: true
    orcid: 0000-0002-0751-2864
    email: franchak@ucr.edu
    affiliations:
      - id: id1
        name: University of California, Riverside
        department: Department of Psychology

      
author-note:
  blank-lines-above-author-note: 1
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Example: This study was registered at ClinicalTrials.gov (Identifier NTC998877).
    study-registration: null
    # Acknowledge and cite data/materials to be shared.
    data-sharing: null
    # Example: This article is based on data published in Pulaski (2017).
    # Example: This article is based on the dissertation completed by Graham (2018).    
    # Example: Sally Jones earns royalties from the sale of Test X.
    conflict-of-interest: null
    # Example: This study was supported by Grant A123 from the National Science Foundation.
    financial-support: null
    # Example: The authors are grateful for the technical assistance of Dr. X during the initial design and setup of our lab equipment.
    gratitude: null
    # Example. Because the authors are equal contributors, order of authorship was determined by a fair coin toss.
    authorship-agreements: null
abstract: "XXX"
keywords: [infant, restraint, everyday experiences, wearable sensors, machine learning]
bibliography: [references.bib]
format:
  apaquarto-docx: 
    floatsintext: false
  apaquarto-html: default
  apaquarto-typst: default
  apaquarto-pdf: 
    # can be jou (journal), man (manuscript), stu (student), or doc (document)
    # for now, tables and figures do not render properly in jou mode. 
    documentmode: man
    # can be 10pt, 11pt, or 12pt
    fontsize: 12pt
    # Integrate tables and figures in text
    floatsintext: false
    # a4 paper if true, letter size if false
    a4paper: false
    # suppresses loading of the lmodern font package
    nolmodern: false
    # Suppresses loading of the fontenc package
    nofontenc: false
    # Suppress the title above the introduction
    donotrepeattitle: false
    # In jou mode, use times or pslatex instead of txfonts
    notxfonts: false
    # In jou mode, use Computer Modern font instead of times
    notimes: false
    # In jou mode, cancels automatic stretching of tables to column width 
    notab: false
    # Uses Helvetica font in stu and man mode
    helv: false
    # In man and stu mode, neutralizes the \helvetica command
    nosf: false
    # In man and stu mode, uses typewriter font
    tt: false
    # Puts draft watermark on first page
    draftfirst: false
    # Puts draft watermark on all pages
    draftall: false
    # Masks references that are marked as the author's own
    mask: false
    journal: null
    volume: null
    course: null
    professor: null
    duedate: null
    # Hides correspondence text
    nocorrespondence: false
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
library(papaja)
library(knitr)
library(patchwork)
library(scales)
library(ggforce)
library(hms)
library(tidyverse)
library(lubridate)
library(rstatix)
library(janitor)
library(flextable)
library(effectsize)
library(ggbeeswarm)
library(DescTools)
```

```{r}
#| include: false

set.seed(2025)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

theme_update(text = element_text(size = 11),
             axis.text.x = element_text(size = 11, color = "black"),
             axis.title.x = element_text(size = 12),
             axis.text.y = element_text(size = 11,  color = "black"),
             axis.title.y = element_text(size = 12),
             panel.background = element_blank(),panel.border = element_blank(),
             panel.grid.major = element_blank(),
             panel.grid.minor = element_blank(), axis.line = element_blank(),
             legend.key = element_rect(fill = "white"))

model_labels <- c("4_no_pos", "16_no_pos", "30_no_pos")
model_shapes <- c(5,1,2) %>% set_names(model_labels)
model_colors <- c("#0072B2","#E69F00", "#009E73") %>% set_names(c("4 s", "16 s", "30 s"))
class_labels <- c("r", "u")
class_shapes <- c(5,1) %>% set_names(class_labels)
# class_colors <- c("#0072B2","#E69F00") %>% set_names(class_labels)

indx <- c("accuracy","kappa", "sensitivity","pos_pred_value","f1")
per_class_indx <- c("sensitivity","pos_pred_value","f1")

```

```{r}
# demographics
load("whole-model-prediction.RData")
rm(ds, ds_sum_excl, ds_sum_excl0, id_demo, id_info)
ds_sum <- ds_sum %>% 
  filter(unique_id != "185/3")
ds_human <- ds_human %>% 
  filter(unique_id != "185/3")
demo <- read.csv("lena-imu-compiled.csv")
demo[demo$id=="180", "infant_sex"] <- "male"

temp <- ds_sum
temp <- temp %>% 
  separate(unique_id, into=c("id","sess"),sep="/",convert = TRUE) %>% 
  distinct(id, .keep_all = TRUE)

temp <- left_join(temp, demo %>% select("id","infant_sex","infant_race","infant_ethnicity") %>% distinct(id, .keep_all = TRUE), by="id", unmatched="drop")

age<-temp %>% 
  group_by(age_group) %>% 
  summarize(n=n(),
            .groups="drop")

sex<-temp %>% 
  group_by(infant_sex) %>% 
  summarize(n=n(),
            .groups="drop")
race<-temp %>% 
  group_by(infant_race) %>% 
  summarize(n=n(),
            .groups="drop")

```

```{r}
# Human coding reliability
colnames(ds_human) <- c("unique_id","agree","kappa")
library(psych)
stats_human <- ds_human %>% 
  summarize(mean_agree = mean(agree),
            min_agree = min(agree),
            max_agree = max(agree),
            mean_kappa = mean(kappa),
            min_kappa = min(kappa),
            max_kappa = max(kappa))
```

[NEED TO BE INCORPORATED WITH WORD DOC]

Physical restraint is a common feature in infants’ everyday life. Infants are routinely held or carried by caregivers, or restrained in devices like highchairs, walkers, and strollers for the sake of comforting, safety, transportation, etc [@Birken2015s]. Restraint influences infants’ motor development by constraining their body movement [e.g., @Pin2007z]. Restraint also frames infants’ learning experiences for perceptual and cognitive development. For example, the frequency of object holding experiences differs between restrained and unrestrained times [@Franchak2024l], the amount and quality of language infants receive decrease as infants spend more time restrained in devices [@Malachowski2023a], yet the frequency of joint attention and

Holding and carrying restraint practices may facilitate infants' social cognitive development [e.g., @Berecz2020y]. Restraint, as an externally regulated aspect of infants’ daily life, decides the scope of input and experiences infants have access to. Therefore, characterizing infants restrained experiences helps profile infants’ daily learning contexts and situate their motor, perceptual, and cognitive development within everyday experiences.

Previous studies made several attempts to document infant restrained time at home. Some used parental surveys, including questionnaires, diaries, or ecological momentary assessment [e.g., @Abbott2001u; @Karasik2018t; @Franchak2019o]. Some used video recording and human annotation [e.g., @Wang2025v]. However, these methods cannot provide a continuous and prolonged account of infants’ experiences. In the current study, we propose that wearable sensors can be a solution for recording continuous, full-day movement data. Moreover, previous studies had different emphases on restraint practices. Some only focused on caregiver restraint like holding and carrying [e.g., @Airaksinen2024z], some only focused on seating devices use [e.g., @Callahan1997s], some focused broadly on containment equipment [e.g., @Bartlett2003w; @Carson2022n; @Hesketh2015i; @Karasik2022a], and others covered a wider range of restraint like furniture placement, seating devices and holding [e.g., @Siddicky2020f; @Franchak2024l; @Malachowski2023a]. In the current study, we combine different aspects of restraint that have been separately examined into one investigation.

## Previous Measurements of Restrained Experiences

Despite different emphases on infant restrained experiences, past research shows that both caregiver restraint and device restraint are common in infant daily life, and the time infants spend restrained changes over development. In terms of caregiver restraint, infants aged from 2 to 6 months spent around 40% of their awake time being held or carried [@Malachowski2023a; @Siddicky2020f]. Older infants aged from 10 to 13 months spent around 19% of their awake time being held or carries [@Franchak2024l]. In terms of device restraint, infants aged from 4 to 6 months spent around 20% of awake time in seating devices, and 35% in placement with physical support [@Callahan1997s; @Malachowski2023a]. Older infants aged from 10 to 13 months spent around 27% of awake time in restraining furniture or devices [@Franchak2024l].

Based on the statistics above, it appears that caregiver restraint and device restraint decrease with age. However, other work that compared across ages within a single study presented contradictory results. @Airaksinen2024z, using wearable sensors and machine learning modeling, found that infants from Finland experienced increased holding and carrying time from 4 to 18 months of age. On the contrary, @Carson2022n, using parental retrospective report, found that infants from Canada experienced decreased overall restrained time from 2 to 6 months of age. @Franchak2019o, using ecological momentary assessment (EMA), also found that infants from the US experienced decreased time held by caregivers from 3 to 12 months. The author and their colleagues further expanded the findings to 10- to 13-month-olds, showing that infants experienced decreased held time and increased overall unrestrained time with age [@Franchak2024l]. The discrepancies may be attributed to study design (i.e., longitudinal versus cross-sectional), cultural differences, as well as research methods (i.e., wearable sensors versus parental reports). The current study, which uses wearable sensors on U.S. infants aged from 4 to 14 months, might help clarify the inconsistencies found in prior research.

## Restrained Practices Shape Infant Learning Opportunities in Real Time

Beyond measuring total duration of infants’ restrained time, prior research also examined how real-time restrained experiences may constrain or support infant learning. On the one hand, restraint practices limit infants’ opportunities for learning. First, restraint practices prevent infants from moving their body parts (e.g., spine, legs, hip) and thus may cause temporary delay in motor development [@Abbott2001u; @Bartlett2003w; @Carson2022n; @Karasik2023d; @Pin2007z; @Siddicky2020f]. Second, infants with relatively more restrained time may produce fewer object interactions, with longest caregiver restrained periods related to the least object interaction [@Franchak2024l]. Third, infants with relatively more device-restrained time may hear less amount of and less consistent adult language [@Malachowski2023a].

On the other hand, restrained periods can enrich certain kinds of learning, especially for younger infants, through stabilizing their body posture. Consider young infants who cannot sit up independently yet and thus cannot free up their hands to explore objects [@Soska2010t]. Restraint devices allow these pre-sitting infants to experience the advantages of a sitting posture by steadying their bodies and enabling eye-hand coordination. Moreover, infants in sitting or upright positions have better opportunities to look at faces compared to infants in prone position [@Franchak2018r]. Restraint devices or carriers that rescue infants from prone position can elevate their viewpoint and afford richer visual input. These examples demonstrate that restraint practices might provide infants with learning experiences that they would not otherwise obtain due to their body constraints, especially for younger infants. These examples also underscore that real-time observation of restraint is essential for understanding how context shapes infants’ multimodal learning opportunities.

## Limitations of Previous Research Methods

Researchers have used survey methods including traditional retrospective report and ecological momentary assessment (EMA) to quantify infants’ restrained time, but both methods have similar limitations. Retrospective parental report, which was most commonly used, asks parents to estimate their infant’s average daily restrained time over the past few weeks or month [@Abbott2001u; @Bartlett2003w; @Callahan1997s; @Carson2022n; @Hesketh2015i; @Karasik2018t; @Siddicky2020f]. Despite its widespread use, this method may not be precise enough because it highly relies on parents’ memory—it is difficult to recall and sum every bout of restraint practices and then report an overall time estimation [@Bradburn1987g]. There are exceptions where caregivers were asked to complete structured daily diaries reflecting on the previous day’s restraint practices in short intervals (e.g., every five minutes) [@Karasik2018t; @Majnemer2005j]. However, this method might still be sensitive to memory loss and thus not reliable in providing accurate data.

The recently developed method EMA reduces caregivers’ memory burden by making momentary phone-survey prompts to caregivers multiple times (e.g., 5, 10, or 12 times) randomly throughout a day [@Franchak2019o; @Franchak2024l; @Malachowski2023a]. However, same limitations still remain. First, both methods assume that the respondent is consistently present to observe the infant throughout the day. The report might not be fully representative of full-day activities if the infant happens to be restrained more frequently when the respondent is absent (e.g., the caregiver restrains the infant at home while they goes out on errands). Second, neither method provides continuous time-series data. They cannot answer granular questions such as: When throughout the day does restraint happen most frequently? Are restrained bouts usually prolonged or brief? Thus, a method not relying on parental report and providing time-sequence information is needed.

Video recording and human annotation, as another method, overcomes the above drawbacks of survey-based methods. However, one of the limitations of this method is being obtrusive. The presence of a researcher in the room may affect the caregiver’s and infant’s behaviors. Another limitation is that this approach is labor-intensive and time-consuming, which limits its use in large-scale or long-duration studies. Indeed, these studies usually do not have long duration of recording (e.g., one hour in @Wang2025v). Within the short duration of recording, researchers would tend to record the period when the infants are awake and active. In contrast, the periods when the infants are restrained, like seated in front of TV or in highchairs, would likely be skipped. Thus, this method is sensitive to sampling bias and might not be representative of infants’ whole-day experiences.

## Promises and Challenges of Wearable Sensors

A system that uses wearable sensors (i.e., inertial movement units) to record real-time movement data and machine learning models to classify movement categories can overcome all the limitations listed above. First, by collecting infants’ movement data through sensors, the system can objectively capture infants’ position, not relying on parental report. Second, the sensors can provide moment-to-moment observation thatd is continuous and rich in time-sequence information. Third, the sensors are embedded in baby garments, making the system less obtrusive. Fourth, the system can record and process full-day or multiple-day data at a time due to the sensors’ sufficient battery life and automative classification of machine learning models.

The past decade has witnessed an uptake of using wearable sensors to record and classify movement categories, but challenges remain. The application of wearable sensors progressed from adults [@Arif2015u; @Preece2009n], to children [@Nam2013y; @Ren2016-px; @Stewart2018h], and then to infants [@Airaksinen2020j; @Airaksinen2024z; @Franchak2021k; @Franchak2024e; @Yao2019q]. For infants, there have been established models that classify infants’ body postures [@Airaksinen2020j; @Franchak2021k; @Franchak2024e], and models that detect holding and carrying periods [@Airaksinen2024z; @Yao2019q]. Prior works concluded valuable experience, such as using multiple sensors, and including gyroscope data together with accelerator data [@Airaksinen2025l].

However, one of the challenges researchers continue to face is the reduced model performance in naturalistic home settings [@Airaksinen2024z; @Franchak2024e; @Yao2019q], compared to more controlled laboratory environments [@Airaksinen2020j; @Franchak2021k]. Indeed, infants may exhibit more complex and fluid body positions at home versus in lab, which requires the models to be able to generalize across scenarios. Moreover, since no prior work has used wearable sensors to detect infants’ overall restrained periods beyond caregiver restraint, the current study faces a unique challenge to detect this new behavior category.

Detecting restrained periods would be even more challenging than classifying body postures or detecting holding periods only. First, infants may show similar body movements whether restrained or not. For example, infants would kick legs when they are lying without restraint and also when they are strapped in a swing. The differences in body movement between restrained and unrestrained periods may be subtle to capture. Possible distinctions might be manifested in reduced range of motion or repetitive motion due to constraints, which requires high-precision sensors and sensitive models to detect. Second, infants may present completely different body movements across various restrained scenarios. Holding is relatively easy to detect because it produces distinguishable patterns such as infants getting lifted or put down at an unusual speed, and infants moving at an adult’s pace with minimal leg movement. However, other kinds of restraint have diverse movement patterns. For example, infants seated in highchairs would be in a sitting position with occasional limbs swinging, whereas infants positioned in walkers would be in an upright position with extended legs and slow walking. Accordingly, the wearable sensors system must identify the underlying kinematic signatures common to various restraint types while accounting for their contextual variability.

A promising strategy to address this untested classification lies in the flexibility of modeling approaches. One strategy is adjusting the window size within which movement data are aggregated. With a fixed sampling rate, larger windows provide more data for the model to leverage to predict the current category. Prior studies have used window sizes ranging from 2.3 to 4 seconds for infant posture category—a multiclass classification [@Airaksinen2020j; @Franchak2024e], and from 1.15 to 10 seconds for infant holding and carrying detection—a binary task [@Airaksinen2024z]. Considering that transitions between restraint states are typically slower than transitions between postures, we set window sizes longer than 4 seconds. Theoretically, shorter windows have higher temporal resolution and are sensitive to abrupt changes but may not be robust enough to make holistic judgement. In contrast, longer windows can smooth noises and are more informative but may not be sensitive enough or may introduce irrelevant data from heterogeneous activities. Determining the optimal window size is therefore a key step for the model to reliably detect infants’ restrained periods.

## Current Study

In short, we propose that wearable sensors offer a more objective, unobtrusive, information-rich, and efficient approach to capturing infants’ full-day experiences in natural settings compared to prior methods. However, unique challenges remain in our new effort to detect infants’ restrained periods. We used an existing data set with infants aged from 4 to 14 months. Infants wore a special garment with four sensors tucked in pockets—two on each side of thighs and two on each side of ankles. In each visit, the family received the equipment dropped off at their doorstep in the morning. The caregiver was instructed to put the garment on the infant to record their whole-day movement and set up a third-person camera to record the first 1.5 hours of the study as video ground truth for human annotation. The current study tested whether a supervised machine learning model trained on human annotated labels can accurately classify infants’ restrained status. We varied the window size as 4s, 16s, and 30s and tested the model performance.

We tested the model in two ways. First, we compared how accurately model predictions matched human annotation for the 1.5 hours of the video-recorded session. We defined \>85% agreement as a good accuracy, based on prior studies reporting 85–96% accuracy in classifying infants’ movement at home. Second, we applied the model to full-day data to estimate the proportion of time each infant was restrained. We tested the convergent validity of the model against past research to determine whether restrained time decreased with age [@Carson2022n; @Franchak2019o; @Franchak2024l].

# Method

## Dataset

The data set was published on Databrary (Franchak, 2023) and used in other empirical works (XX). The data set consists of two cohorts of infants: `r apa_num(age[1,2])` younger infants aged from 4 to 7 months of age, and `r apa_num(age[2,2])` older infants aged from 11 to 14 months of age. Among the total of `r apa_num(age[1,2]+age[2,2])` infants, 26 of them participated in the study for only once, while 39 of them participated from 2 to 4 times, resulting in a total of 142 sessions. Thirty-four of the infants are female and thirty-one are male. Caregivers reported the infants’ race as White (*n* = 23), Hispanic or Latinx (*n* = 8), African American (*n* = 2), Asian (*n* = 2), more than one (*n* = 15), or not fitting into the above races or chose not to answer (*n* = 15). Families were recruited from Southern California via social media and community recruitment events and received \$30 for compensation for each visit. Caregivers provided informed consent before each session of the study. The whole procedures were approved by the University of California, Riverside Institutional Review Board.

## Apparatus

As shown in figure XX, a special infant garment was customized to secure four lightweight wearable sensors (MC10 Biostamp) at four specific locations (i.e., right thigh, left thigh, right ankle, and left ankle). Each sensor collects raw data as accelerometer signals (in x, y, and z orientations) and gyroscope signals (in roll, pitch, and yaw orientations). The sampling rate of 62.5 Hz allows for millisecond-level granularity, and the sufficient battery life and built-in storage allows for whole-day-long recording. An action camera fixed on a mini tripod (GoPro HERO9 Black) was used to record a third-person video of the first 1.5 hours into the session as ground truth. The camera and sensors were synchronized at the beginning of each session.

## Procedure

On the day of a session, a researcher arrived at the family’s house in the morning with a bucket of all equipment. The researcher turned on the camera and synchronized it with the sensors by dropping the garment in front of the camera—the action would leave a distinctive timestamp on both the camera and the sensors. The researcher then called the caregiver to pick up the bucket and instructed the caregiver to set up the equipment. The caregiver was told to place the camera to where it could capture the main room and to move the camera if needed to ensure the infant was always in view. The caregiver was instructed to put the garment on the infant and position the infant in various postures (including supine, prone, sitting, crawling, standing, walking, being held, and being restrained in seating devices) for at least one minute each for model training purpose. Next, the caregiver played with the infant as usual in front of the camera for 10 minutes. Afterwards, the family went about their day with the garment on the infant. The caregiver logged the time when the infant napped or that the garment was removed for reasons such as diaper changes, baths, going out on errands, etc. The researcher picked up the equipment the next day.

## Human Annotation of Restraint

Trained human coders watched the 1.5-hour videos and annotated the periods when the infant was restrained or unrestrained using Datavyu software (https://datavyu.org/). Restrained and unrestrained is defined as a binary category, meaning that the infant could only be in either of the two statuses unless they were out of view. We define restrained periods as when infants cannot initiate or control their own movements, which includes being held or carried by caregivers, or being restrained in devices that constrain them from changing body position. Unrestrained periods, on the contrary, are when infants could freely change their body position or locomote. Each video file was coded by two coders: one primary coder annotated the whole 1.5-hour video, and an independent reliability coder annotated the first 30 minutes of the video. The inter-rater reliability was calculated as the proportion of video frames where the two coders coded the same label. The human annotation reached a high agreement with an overall accuracy of `r apa_num(stats_human[1,1]*100)`%, ranging from `r apa_num(stats_human[1,2]*100)`% to `r apa_num(stats_human[1,3]*100)`%, and an average kappa of `r apa_num(stats_human[1,4])`, ranging from `r apa_num(stats_human[1,5])` to `r apa_num(stats_human[1,6])`.

## Model Classification of Restraint

The machine-learning classification process followed procedures similar to [@Franchak2021k; @Franchak2024e]. First, we aligned the time series of human annotations and sensor data by matching the synchronization time stamp in the video and sensors. This resulted in a data set with human-coded labels and motion data from four sensors—each capturing acceleration (x, y, z) and angular velocity (roll, pitch, yaw)—available at each sampling point.

Building on previous methods, we focused on short temporal windows to capture the dynamic nature of movement. Accordingly, we aggregated raw IMU data into motion features within sliding windows of n seconds. For each window, we computed statistical features—mean, median, minimum, 25th percentile, 75th percentile, maximum, skewness, kurtosis, standard deviation, and sum—for all six signals across the four sensors, resulting in 240 features. We then added 196 additional features based on cross-sensor and cross-orientation metrics such as correlations and pairwise differences, resulting in 436 features per window.

We started the windowing process from the syncing point and slide the window every *n*/2 seconds. Consequently, each time point spaced every *n*/2 seconds contains 436 motion features, aggregated from the *n*/2 seconds before and after that point. For model training and testing, we included only time windows where the infant remained in a single status, so that each window could be assigned one unambiguous label. The window size *n* varied from 4s, to 16s and 30s.

The third step was to train and validate supervised machine learning models that classify the motion features into restrained status. We applied the random forest algorithm (Breiman, 2001) in modeling using the *DecisionTree* package (Sadeghi et al., 2022) in Julia. We used a “training” set of the windowed data set and validated the models by comparing their predictions on a separate “testing” set to the corresponding human annotations. Specifically, we used a leave-one-out cross-validation approach—each session was held out once and only as the testing set—to evaluate how well the model generalized to unseen data. After validation, an optimal window size was picked and a final model was trained using all the human annotations to process whole-day sensor data of all sessions.


# Results
We first compared the model’s performance to the human annotation of the first 1.5 hours across all 142 sessions using leave-one-out cross-validation approach. We reported performance metrics including accuracy, kappa, sensitivity, positive predictive value, F1, and the correlation of overall estimate between the modal and human annotation. We also examined prediction biases of the model by looking at class-specific metrics--sensitivity and positive predictive value--and the prediction error of the overall estimate. Second, we applied the model to process infants’ full day sensor data and examined the model’s convergent validity by comparing its estimate and age-related change to previous works.

## Model Prediction Compared to Human Annotation

```{r}
load("matrices.RData")
matrix_comb <- rbind(matrix_30, matrix_16, matrix_4)
matrix_comb <- matrix_comb %>%  
  clean_names() %>% 
  mutate(model = factor(model, levels=c("4_no_pos","16_no_pos","30_no_pos")),
         positive_class = factor(positive_class, levels=c('r','u'))) %>% 
  mutate(prevalence_df = detection_prevalence - prevalence)

```

```{r tbl-indexbetweenmodels}
#| tbl-cap: Performance metrics according to window lengths
#| ft.align: left
#| apa-note: F, df, and p value indicate one-way ANOVA on each metric comparing across the three window lengths 

# Compare between models
# Descriptive
table_mean <- matrix_comb %>%
  group_by(positive_class, model) %>%
  summarise(
    mean_acc = mean(accuracy, na.rm = TRUE),
    mean_kappa = mean(kappa, na.rm = TRUE),
    mean_sen = mean(sensitivity, na.rm = TRUE),
    mean_ppv = mean(pos_pred_value, na.rm = TRUE),
    mean_f1 = mean(f1, na.rm = TRUE),
    .groups="drop"
  ) %>%
  pivot_longer(mean_acc:mean_f1, names_to = "Metric", values_to = "mean") %>%
  pivot_wider(names_from = model, values_from = mean) %>% 
  mutate(Metric = recode(Metric,
                        "mean_acc" = "Accuracy",
                        "mean_kappa" = "Kappa",
                        "mean_sen" = "Sensitivity",
                        "mean_ppv" = "PPV",
                        "mean_f1" = "F1"),
         positive_class = recode (positive_class,
                                  "r" = "Restrained",
                                  "u" = "Unrestrained")) %>%
  relocate("Metric", .before = "positive_class") %>% 
  mutate(Metric = factor(Metric, levels=c("Accuracy", 
                                            "Kappa",
                                            "Sensitivity", 
                                            "PPV", 
                                            "F1"))) %>% 
  arrange(Metric) %>% 
  rename("Category" = "positive_class",
         "4 s" = "4_no_pos",
         "16 s" = "16_no_pos",
         "30 s" = "30_no_pos")

# Between models: ANOVA
aov_mdl <- function(index, ctgr){
  temp_ds <- matrix_comb %>% filter(positive_class==ctgr)
  fmla <- reformulate("model", response = index)  # response ~ model
  anova <- tidy(aov(fmla, data = temp_ds))
}
aov_r <- map(indx, ~aov_mdl(index=.x, ctgr="r")) %>% 
  map2_dfr(indx, ~mutate(as.data.frame(.x), Metric = .y)) %>% 
  mutate(Category = "r")
aov_u <- map(indx, ~aov_mdl(index=.x, ctgr="u")) %>% 
  map2_dfr(indx, ~mutate(as.data.frame(.x), Metric = .y)) %>% 
  mutate(Category = "u")
table_aov <- rbind(aov_r, aov_u) %>% 
  mutate(df_resi = lead(df, n = 1)) %>% 
  filter(term=="model") %>% 
  mutate(df = str_glue("({df}, {df_resi})")) %>% 
  select(-term, -df_resi, -sumsq, -meansq) %>% 
  rename("F" = "statistic",
         "p" = "p.value") %>% 
  relocate("Metric", .before = "df") %>% 
  relocate("Category", .before = "df") %>% 
  relocate("df", .before = "p") %>%
  mutate(Metric = recode(Metric,
                        "accuracy" = "Accuracy",
                        "kappa" = "Kappa",
                        "sensitivity" = "Sensitivity",
                        "pos_pred_value" = "PPV",
                        "f1" = "F1")) %>%
  mutate(Metric = factor(Metric, levels=c("Accuracy", 
                                            "Kappa",
                                            "Sensitivity", 
                                            "PPV", 
                                            "F1"))) %>% 
  arrange(Metric)

# Combine descriptive and ANOVA results and put them in a flex table
table_btw_mdl <- cbind(table_mean, table_aov %>% select(-Metric, -Category)) %>% 
  slice(-2,-4) %>% 
  mutate(p = ifelse(p < .001,"<.001",round(p,3)),
         p = str_remove(p, "^0")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3)))

table_btw_mdl[1,2] <- ""
table_btw_mdl[2,2] <- ""

table_flex_btw_mdl <- table_btw_mdl %>% 
  flextable() %>% flextable::theme_apa() %>% line_spacing(part = "all") %>% 
  merge_v(j = "Metric") %>% fix_border_issues() %>% 
  hline(i = c(1,2,4,6,8)) %>% 
  padding(padding.top = 2, padding.bottom = 2) %>% 
  set_table_properties(layout = "autofit", width = .99) %>% 
  align(align = "center", part = "all")
table_flex_btw_mdl
```

```{r tbl-cibtwctg}
# For per-class metrics (i.e., sens, ppv, f1), calculate the CI of the metric difference btw classes (i.e., r and u)
# Don't need an APA table for this, but need the table to call values from
# Function to bootstrap CI of gap for each model and each metric
bootstrap_gap <- function(df, metric, B = 2000, gap_cal = F) {
  if (gap_cal == F){
    df_wide <- df %>%
    select(session, id, positive_class, !!sym(metric)) %>%
    pivot_wider(names_from = positive_class, values_from = !!sym(metric)) %>%
    mutate(gap = r - u,
           id_session = paste(id, session, sep = "_"))
  } else{
    df_wide <- df %>% 
      mutate(id_session = paste(id, session, sep = "_"))
  } # if gap column is calculated or not
  
  sessions <- unique(df_wide$id_session)
  n <- length(sessions)
  res <- numeric(B)
  
  for (b in seq_len(B)) {
    sampled <- sample(sessions, size = n, replace = TRUE)
    df_b <- df_wide %>% filter(id_session %in% sampled)
    res[b] <- mean(df_b$gap, na.rm = TRUE)
  }
  
  ci <- quantile(res, c(0.025, 0.975), na.rm = TRUE)
  
  list(
    metric = metric,
    mean_gap = mean(df_wide$gap, na.rm = TRUE),
    ci = ci
  )
}

table_btw_ctg <- crossing(model = model_labels, metric = c("sensitivity","pos_pred_value")) %>%   # all combinations
  mutate(result = map2(model, metric, 
                       ~ bootstrap_gap(df = matrix_comb %>% filter(model == .x), 
                                       metric = .y))) %>%
  mutate(result = map(result, ~ tibble(
    mean_gap = .x$mean_gap,
    ci_low   = .x$ci[1],
    ci_high  = .x$ci[2]
  ))) %>% 
  unnest(result) %>% 
  mutate(model = factor(model, levels = c("4_no_pos","16_no_pos","30_no_pos")),
         metric = factor(metric, levels = c("sensitivity","pos_pred_value"))) %>% 
  mutate(model = recode(model,
                        "4_no_pos" = "4 s",
                        "16_no_pos" = "16 s",
                        "30_no_pos" = "30 s"),
         metric = recode(metric,
                         "sensitivity" = "Sensitivity",
                         "pos_pred_value" = "PPV")) %>% 
  arrange(model, metric) %>% 
  rename("Window Length" = "model",
         "Metric" = "metric",
         "M"="mean_gap",
         "95%CI lower bound"="ci_low",
         "95%CI upper bound"="ci_high")
# 
# table_flex_btw_ctg <- table_btw_ctg %>% 
#   flextable() %>% flextable::theme_apa() %>% 
#   line_spacing(part = "all") %>%
#   merge_v(j = "Window Length") %>% fix_border_issues() %>% 
#   add_header_row(values = c("","", "Difference (rest.- unrest.)"), colwidths = c(1,1,3)) %>% 
#   hline(i = c(2,4,6), part = "body") %>%
#   padding(padding.top = 2, padding.bottom = 2) %>% 
#   set_table_properties(layout = "autofit", width = .99) %>% 
#   align(align = "center", part = "all")
# table_flex_btw_ctg
```

```{r fig-cibtwctg}
#| fig-cap: Differences in sensitivity and PPV between restrained and unrestrained prediction according to window lengths
#| fig-width: 11
#| fig-height: 5
#| apa-note: Each vertical line indicates the 95%CI of the metric's difference between restrained prediction and unrestrained prediction. The dot on each line denotes the mean of the difference.

p_ci_sen <- ggplot (table_btw_ctg %>% 
                      filter(Metric=="Sensitivity"),
                    aes(x=`Window Length`, 
                        y = `M`,
                        ymin = `95%CI lower bound`, 
                        ymax = `95%CI upper bound`, 
                        color = `Window Length`))+
  geom_pointrange(position = position_dodge(width = 0.5), size = 0.7) +
  geom_segment(aes(x = 0, xend = 3.5, y = 0, yend = 0),
             linetype = "dashed", color = "black")+
  scale_color_manual(values=model_colors,labels=c("4 s","16 s", "30 s")) +
  scale_y_continuous(
    limits = c(-0.15, 0.05),
    breaks = c(-0.15,-0.1, -0.05, 0, 0.05)
  )+
  theme_update() +
  labs(x = "",y = "Metric value of rest. - unrest.")+
  guides(color="none")+
  ggtitle("A. Sensitivity value difference")+
  annotate("text", x = 2.6, y = -0.13, label="The model is more \n sensitive in detecting \n unrestrained bouts",color = "black")
p_ci_sen

p_ci_ppv <-ggplot (table_btw_ctg %>% 
                      filter(Metric=="PPV"),
                    aes(x=`Window Length`, 
                        y = `M`,
                        ymin = `95%CI lower bound`, 
                        ymax = `95%CI upper bound`, 
                        color = `Window Length`))+
  geom_pointrange(position = position_dodge(width = 0.5), size = 0.7) +
  geom_segment(aes(x = 0, xend = 3.5, y = 0, yend = 0),
             linetype = "dashed", color = "black")+
  scale_color_manual(values=model_colors,labels=c("4 s","16 s", "30 s")) +
  scale_y_continuous(
    limits = c(-0.05, 0.1),
    breaks = c(-0.05, 0, 0.05, 0.1)
  )+
  theme_update() +
  labs(x = "",y = "Metric value of rest. - unrest.")+
  guides(color="none")+
  ggtitle("B. PPV value difference")+
  annotate("text", x = 2.6, y = 0.085, label="The model's prediction \n of restrained bouts \n is more reliable",color = "black")
p_ci_ppv

p_ci_sen | p_ci_ppv
# p_ci <- ggplot(table_btw_ctg, 
#        aes(x = Metric, 
#            y = `M`, 
#            ymin = `95%CI lower bound`, 
#            ymax = `95%CI upper bound`, 
#            color = `Window Length`)) +
#   geom_pointrange(position = position_dodge(width = 0.5), size = 0.7) +
#   geom_segment(aes(x = 0, xend = 3, y = 0, yend = 0),
#              linetype = "dashed", color = "gray")+
#   scale_color_manual(values=model_colors,labels=c("4 s","16 s", "30 s")) +
#   scale_x_discrete(expand = expansion(add = c(0,2)))+
#   scale_y_continuous(
#     limits = c(-0.15, 0.1),  
#     breaks = c(-0.15,-0.1, -0.05, 0, 0.05, 0.1) 
#   )+
#   theme_update() +
#   theme(legend.position = c(0.89,0.1))+ 
#   labs(x = "",y = "Metric value difference (rest.- unrest.)",
#     color = "Window Length")+
#   annotate("text", x = 3.5, y = -0.05, label="Unrestrained prediction \n has higher \n metric's value",color = "black")+
#   annotate("text", x = 3.5, y = 0.05, label="Restrained prediction \n has higher \n metric's value",color = "black")

```

```{r tbl-prevalencecor}
#| apa-note: Prevalence was calculated as the proportion of restrained periods among all window samples for each session. Prediction error was calculated as model prediction prevalence - human annotation prevalence. A negative prediction error means an underestimate from model prediction. M = mean difference; CI = bootstrapped confidence interval. 
#| tbl-cap: Model predicted and human annotated prevalence of restrained periods, their correlation, and differences
#| ft.align: left

df_prev <- matrix_comb %>%
  filter(positive_class=="r") %>%
  select(model, id, session, detection_prevalence, prevalence) %>% 
  mutate(gap = detection_prevalence-prevalence)

prev_gap <- map(model_labels, ~ bootstrap_gap(df = df_prev %>% filter(model == .x), metric = "gap", gap_cal = T))

table_prev <- df_prev %>%
  group_by(model) %>%
  summarise(actual_prev = mean(prevalence),
            prev = mean(detection_prevalence),
            cor = cor (detection_prevalence, prevalence),
            .groups = "drop") %>% 
  mutate(gap = map(prev_gap, ~ tibble(
    mean_gap = .x$mean_gap,
    ci_low   = .x$ci[1],
    ci_high  = .x$ci[2]))) %>%
  unnest(gap) %>% 
  mutate(model=recode(model,
                      "4_no_pos" = "4 s",
                       "16_no_pos" = "16 s",
                       "30_no_pos" = "30 s"))

table_prev_100 <- table_prev %>% 
  mutate(actual_prev=actual_prev*100,
         prev = prev*100, 
         mean_gap = mean_gap *100,
         ci_low=ci_low * 100,
         ci_high=ci_high*100) %>% 
  mutate(ci = paste0("[",round(ci_low, 2),", ", round(ci_high, 2), "]" )) %>% 
  select(-ci_low, -ci_high) %>% 
  rename("Window Length" = "model",
         "Human Annotated" = "actual_prev",
         "Model Predicted" = "prev",
         "Correlation (r)" = "cor",
         "M" = "mean_gap",
         "95%CI" = "ci")

table_flex_prev <- table_prev_100 %>% 
  flextable() %>% flextable::theme_apa() %>% line_spacing(part = "all") %>% 
  fix_border_issues() %>% 
  add_header_row(values = c("","Prevalence (%)", "","Prediction Error (%)"), colwidths = c(1,2,1,2)) %>% 
  hline(i = c(1,2,3)) %>% 
  padding(padding.top = 2, padding.bottom = 2) %>% 
  set_table_properties(layout = "autofit", width = .99) %>% 
  align(align = "center", part = "all")
table_flex_prev
```

```{r fig-prevalencegap}
#| fig-cap: XX
#| fig-width:11
#| fig-height: 5
#| apa-note: XX

p_prev_cor <- ggplot(data=)
  
p_prev_gap <- ggplot(data=matrix_comb %>% filter(positive_class=="r") %>%
                   mutate(model=recode(model,
                                       "4_no_pos" = "4 s",
                                       "16_no_pos" = "16 s",
                                       "30_no_pos" = "30 s")),
                 aes(x=model, y=detection_prevalence-prevalence, color=model))+
  geom_violin(trim = FALSE, alpha = 0.4) +
  geom_quasirandom(aes(color = model),
                   dodge.width = 0.7, varwidth = TRUE, alpha = 0.6, size = 2) +
  geom_boxplot(width = 0.1, outlier.shape = NA, fill = "white") + 
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  scale_color_manual(values=model_colors, labels=c("4 s", "16 s","30 s")) +
  labs(x="Window Length", y="Prediction Error (prediction-human annotation)")+
  guides(color = "none")+
  theme_update()
p_prev_gap
```

```{r}
#| eval: false
#| include: false
# check if the data set is imbalanced 
matrix_comb %>% 
  filter(positive_class=="r") %>% 
  group_by(model) %>% 
  summarize(mean_prev = mean(prevalence),
            sd_prev = sd(prevalence),
            .groups="drop")


ggplot(matrix_comb %>% filter(positive_class=="r"), aes(x = prevalence, color=model)) +
  geom_density(alpha = 0.4)

#slightly imbalanced: less unrestrained periods, the ratio (~40%) is similar to the total estimated proportion
```

```{r}
#| eval: false
#| include: false
# check accuracy outliers 
outlier_4 <- matrix_comb %>%
  filter(model == "4_no_pos", positive_class=="r") %>%
  mutate(
    q1 = quantile(accuracy, 0.25),
    q3 = quantile(accuracy, 0.75),
    iqr = q3 - q1,
    is_outlier = accuracy < (q1 - 1.5 * iqr) | accuracy > (q3 + 1.5 * iqr)
  ) %>%  filter(is_outlier)
outlier_16 <- matrix_comb %>%
  filter(model == "16_no_pos", positive_class=="r") %>%
  mutate(
    q1 = quantile(accuracy, 0.25),
    q3 = quantile(accuracy, 0.75),
    iqr = q3 - q1,
    is_outlier = accuracy < (q1 - 1.5 * iqr) | accuracy > (q3 + 1.5 * iqr)
  ) %>%  filter(is_outlier)
outlier_30 <- matrix_comb %>%
  filter(model == "30_no_pos", positive_class=="r") %>%
  mutate(
    q1 = quantile(accuracy, 0.25),
    q3 = quantile(accuracy, 0.75),
    iqr = q3 - q1,
    is_outlier = accuracy < (q1 - 1.5 * iqr) | accuracy > (q3 + 1.5 * iqr)
  ) %>%  filter(is_outlier)

outliers <- rbind(outlier_4,outlier_16, outlier_30)

ggplot(data=outliers, aes(x=session, y=id, color=model))+
  geom_jitter()+
  geom_text(aes(label = paste0("(", id, ", ", session, ")")),
            vjust = -0.5,   # move label a bit above point
            size = 3) 
```

### Overall Model Performance 

@tbl-indexbetweenmodels shows that all window lengths reached high accuracy (`r apa_num(table_btw_mdl[1,3])` for 4 s, `r apa_num(table_btw_mdl[1,4])` for 16 s, and `r apa_num(table_btw_mdl[1,5])` for 30 s), and good kappa value (`r apa_num(table_btw_mdl[2,3])` for 4 s, `r apa_num(table_btw_mdl[2,4])` for 16 s, and `r apa_num(table_btw_mdl[2,5])` for 30 s). All the window lengths also presented high sensitivity (ranging from `r apa_num(min(table_btw_mdl[3,3:5]))` to `r apa_num(max(table_btw_mdl[3,3:5]))` for restrained prediction, and `r apa_num(min(table_btw_mdl[4,3:5]))` to `r apa_num(max(table_btw_mdl[4,3:5]))` for unrestrained prediction), high positive predictive value (ranging from `r apa_num(min(table_btw_mdl[5,3:5]))` to `r apa_num(max(table_btw_mdl[5,3:5]))` for restrained prediction, and `r apa_num(min(table_btw_mdl[6,3:5]))` to `r apa_num(max(table_btw_mdl[6,3:5]))` for unrestrained prediction), and high F1 score (ranging from `r apa_num(min(table_btw_mdl[7,3:5]))` to `r apa_num(max(table_btw_mdl[7,3:5]))` for restrained prediction, and `r apa_num(min(table_btw_mdl[8,3:5]))` to `r apa_num(max(table_btw_mdl[8,3:5]))` for unrestrained prediction). Moreover, the three window lengths did not present significant differences in any of the metrics (*p*s >= `r apa_num(min(table_btw_mdl[1:8,8]))`).

Meanwhile, @tbl-prevalencecor shows that all window lengths provided reliable overall estimate of restrained time. The prevalence of restrained periods the models predicted was strongly correlated with the prevalence annotated by human coders, the correlation coefficient ranging from `r apa_num(table_prev[1,4])` for 4 s and `r apa_num(table_prev[3,4])` for 30 s.

### Bias Towards Unrestrained Class Across Window Lengths

Despite the overall good performance, all window lengths presented prediction bias towards unrestrained class. @tbl-cibtwctg and @fig-cibtwctg show that all window lengths had lower sensitivity and higher positive predictive value (PPV) for restrained compared to unrestrained periods, indicated by the negative difference of sensitivity and positive difference of PPV. The sensitivity difference for 4 s window was `r apa_num(table_btw_ctg[1,3])`, 95% bootstrapped CI [`r apa_num(table_btw_ctg[1,4])`, `r apa_num(table_btw_ctg[1,5])`], for 16 s window was `r apa_num(table_btw_ctg[3,3])`, 95% CI [`r apa_num(table_btw_ctg[3,4])`, `r apa_num(table_btw_ctg[3,5])`], and for 30 s window was `r apa_num(table_btw_ctg[5,3])`, 95% CI [`r apa_num(table_btw_ctg[5,4])`, `r apa_num(table_btw_ctg[5,5])`]. The PPV difference for 4 s window was `r apa_num(table_btw_ctg[2,3])`, 95% CI [`r apa_num(table_btw_ctg[2,4])`, `r apa_num(table_btw_ctg[2,5])`], for 16 s window was `r apa_num(table_btw_ctg[4,3])`, 95% CI [`r apa_num(table_btw_ctg[4,4])`, `r apa_num(table_btw_ctg[4,5])`], and for 30 s window was `r apa_num(table_btw_ctg[6,3])`, 95% CI [`r apa_num(table_btw_ctg[6,4])`, `r apa_num(table_btw_ctg[6,5])`]. The differences indicate that the model tended to under-detect restrained periods but was reliable when labeling a period as restrained. Conversely, it overestimated unrestrained periods, with lower reliability in unrestrained prediction. While none of the window lengths were free from bias, the 30 s window presented a more balanced detection between restrained and unrestrained periods.

The prediction bias was also evident when examining the model’s overall estimate of restrained time and its difference from human annotation. @tbl-prevalencecor and @fig-prevalencegap show a consistent underestimate of restrained periods despite the high correlation of the overall estimate. The 4 s window missed `r apa_num(abs(table_prev[1,5])*100)`% of time that were annotated as restrained (95% CI [`r apa_num(abs(table_prev[1,6])*100)`%, `r apa_num(abs(table_prev[1,7])*100)`%]), the 16 s window missed `r apa_num(abs(table_prev[2,5])*100)`% (95% CI [`r apa_num(abs(table_prev[2,6])*100)`%, `r apa_num(abs(table_prev[2,7])*100)`%]]), and the 30 s window missed the least amount of `r apa_num(abs(table_prev[3,5])*100)`% (95% CI [`r apa_num(abs(table_prev[3,6])*100)`%, `r apa_num(abs(table_prev[3,7])*100)`%]). Taken together, 30 s turned out to be the best window length out of the three, which brought about a balanced performance across classes. We then applied the 30 s model to process the full-day sensor data.


## Model Estimate Compared to Previous Studies

```{r}
# exclude sessions where total recording time is <3 hours
ds_excl <- ds_sum %>% filter(total_time < 3) #11 sessions 
ds_sum_excl <- ds_sum %>% filter(total_time >=3) #132 sessions left
ds_sum_excl <- ds_sum_excl %>% 
  filter(unique_id != "148/1",
         unique_id != "178/3")

stats_fullday <- ds_sum_excl %>% 
  group_by(age_group) %>% 
  summarize(mean_restrained = mean(restrained_prop),
            sd_restrained = sd(restrained_prop),
            .groups="drop")

```

```{r fig-agechange}
#| fig-cap: Infants' full day restrained time proportion with age
#| fig-width: 4
#| fig-height: 3
#| apa-note: Each point denotes one session's restrained time proportion processed by the 30 s model. The box denotes the first (Q1) quartile, the median, and the third (Q3) quartile of prediction error.

t_age <- summary(lm(restrained_prop ~ age_group, data = ds_sum_excl))$coefficients # t test

ds_sum_excl <- ds_sum_excl %>% 
  mutate(age_group_num=ifelse(age_group=="Younger",5.5, 12.5))

p_age <- ggplot(ds_sum_excl, aes(x=agemo, y=restrained_prop)) +
  geom_point(shape = 1,size = 3) + 
  geom_boxplot(
    aes(x = age_group_num, y = restrained_prop, group = age_group_num),
    inherit.aes = FALSE,
    width = 3,
    alpha = 0.2,
    outlier.shape = NA,
    color = "blue"
  ) +
  scale_x_continuous(name = "Age Group", breaks = c(5.5, 12.5), labels=c("Young", "Old"), limits = c(3,15)) +
  scale_y_continuous(name = "Restrained time (%)", breaks = seq(0, 1, .25), limits = c(0,1))+
  theme_update()+
  labs(x="Age Group")

p_age
# ggsave("figure/age_related_change.png",width=6, height=4)
# ggsave("figure/age_related_change.eps",width=6, height=4)

```

In order to compare the model estimate of infants’ full day experience with previous studies, we excluded eleven sessions whose total recording time was less than 3 hours, which were useful for training and validating the models but insufficient for measuring day-long experience. We also excluded another two sessions where the family took the garment off for a substantial portion of time but did not log the time stamp of removal. The remaining 129 sessions have an average recording time of `r apa_num(mean(ds_sum_excl$total_time))` hours (*SD* = `r apa_num(sd(ds_sum_excl$total_time))`).

The model prediction on the 129 full-day sessions converged with previous studies [@Carson2022n; @Franchak2019o; @Franchak2024l]. Younger infants aged from 4 to 7 months spent *M* = `r apa_num(stats_fullday[1,2]*100)`% (*SD* = `r apa_num(stats_fullday[
1,3]*100)`%) of awake time restrained, and older infants aged from 11 to 14 months spent *M* = `r apa_num(stats_fullday[2,2]*100)`% (*SD* = `r apa_num(stats_fullday[2,3]*100)`%) of awake time restrained, which is comparable to the 46% restrained time for infants aged from 10 to 13 months in the previous EMA study [@Franchak2024l]. Moreover, @fig-agechange shows a significant group difference in restrained time between older infants and younger infants (*t* = `r apa_num(t_age[2,3])`, *p* `r apa_num(ifelse(t_age[2,4] < .001, "<.001", t_age[2,4]))`), supporting the convergent validity of the current system.

# Discussions
We validated a system that combines wearable sensors and machine learning models to quantify infants’ restrained experiences in the home environment. The model presented high accuracy and substantial kappa agreement with human-annotated ground truth. The model also presented convergent validity in overall estimate of restraint time and age-related change compared with previous studies. 

We also examined the optimal window length for the classification (4 s, 16 s, and 30 s). All window lengths showed a slight prediction bias towards unrestrained class, which might be due to the inherent imbalance in the training data set—unrestrained periods were more prevalent than restrained periods. However, the 30 s window offered the most balanced detection compared to shorter windows. This advantage might be due to the contextual nature of restraint: infants may exhibit similar body movement whether restrained or not, and a restrained bout usually lasts minutes rather than seconds. A longer window can provide richer contextual information for distinguishing restrained from unrestrained states, whereas shorter windows only capture momentary movements that may appear similar across restrained and unrestrained periods. 

A key contribution of this work is the extension of wearable-sensor methodology to detect infant restraint in naturalistic settings. Wearable sensors have been established as a reliable and unobtrusive approach for capturing full-day, time-series movement data. Previous works validated the use of wearable sensors in classifying infant positions. Our work shows the configuration of four sensors on thighs and ankles and random forest algorithms can also provide sufficiently accurate classification of infant restraint. Although the system cannot be as accurate as human annotation, the trade-off between accuracy and scale favors the sensor-based system, particularly when research questions is oriented at infants’ experiences fluctuating throughout a day or across multiple days. 

With the validated system, researchers can now study infant restrained experiences in a longer time scale and a greater temporal precision. Research questions about the temporal structure of restrained periods can be addressed—for example, how frequently are infants switched between restrained and unrestrained states, or how restrained bouts are distributed across the day or multiple days? Moreover, the system enables investigations linking restraint to concurrent learning experiences: How does the temporal profile of restraint relate to motor development? How does adult language input or infants’ own vocalization patterns shift when they are restrained versus unrestrained?

Moving forward, we can refine the model to differentiate between caregiver restraint and device restraint, as these contexts may differ in the perceptual, social, and cognitive input they afford. Nonetheless, quantifying infants’ general restrained experiences already represents a significant advance. This system provides a powerful tool for capturing infants’ physical contexts and how the contexts shape their daily learning opportunities.

# References
